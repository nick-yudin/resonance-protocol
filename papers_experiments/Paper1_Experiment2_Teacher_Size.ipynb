{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 1: Cross-Architecture Knowledge Transfer via HDC\n",
    "## Experiment 2: Teacher Size Study\n",
    "\n",
    "Tests whether larger teacher models improve HDC transfer quality.\n",
    "\n",
    "**Author**: Nikolay Yudin | **Project**: SEP | **Repo**: github.com/nick-yudin/SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch numpy scikit-learn matplotlib tqdm accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF Login for Llama access\n",
    "from huggingface_hub import login\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    login(token=userdata.get('HF_TOKEN'))\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, numpy as np, json, gc, os, shutil\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'models': {\n",
    "        'distilbert': {'name': 'distilbert-base-uncased', 'type': 'encoder', 'size': '66M'},\n",
    "        'gpt2': {'name': 'gpt2', 'type': 'decoder', 'size': '124M'},\n",
    "        'llama8b': {'name': 'meta-llama/Llama-3.1-8B', 'type': 'decoder', 'size': '8B'},\n",
    "        'qwen14b': {'name': 'Qwen/Qwen2.5-14B', 'type': 'decoder', 'size': '14B'},\n",
    "    },\n",
    "    'train_size': 2000, 'test_size': 500, 'anchor_size': 500,\n",
    "    'hdc_dim': 4096, 'tau': 0.3, 'seeds': [42, 123, 456],\n",
    "    'contrastive_epochs': 20, 'use_4bit': True,\n",
    "}\n",
    "EXPERIMENTS = [('gpt2','distilbert'),('distilbert','distilbert'),('llama8b','distilbert'),('qwen14b','distilbert')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingExtractor:\n",
    "    def __init__(self, model_key, config, device='cuda'):\n",
    "        self.model_key = model_key\n",
    "        info = config['models'][model_key]\n",
    "        self.model_name, self.model_type = info['name'], info['type']\n",
    "        print(f'Loading {model_key} ({info[\"size\"]})...')\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        is_large = 'B' in info['size'] and int(info['size'].replace('B','')) >= 8\n",
    "        if is_large and config.get('use_4bit'):\n",
    "            print('  Using 4-bit quantization...')\n",
    "            bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "            ModelClass = AutoModelForCausalLM if self.model_type == 'decoder' else AutoModel\n",
    "            self.model = ModelClass.from_pretrained(self.model_name, quantization_config=bnb, device_map='auto', trust_remote_code=True)\n",
    "        else:\n",
    "            if self.model_type == 'decoder':\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(self.model_name, trust_remote_code=True).to(device)\n",
    "            else:\n",
    "                self.model = AutoModel.from_pretrained(self.model_name).to(device)\n",
    "        self.model.eval()\n",
    "        self.embed_dim = self.model.config.hidden_size\n",
    "        self.device = device\n",
    "        print(f'✅ {model_key}: {self.embed_dim}d')\n",
    "    \n",
    "    def encode(self, texts, batch_size=8):\n",
    "        all_emb = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f'Encoding {self.model_key}'):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "            dev = self.model.device if hasattr(self.model, 'device') else self.device\n",
    "            inputs = {k: v.to(dev) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                if self.model_type == 'decoder':\n",
    "                    hidden = self.model(**inputs, output_hidden_states=True).hidden_states[-1]\n",
    "                else:\n",
    "                    hidden = self.model(**inputs).last_hidden_state\n",
    "                mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "                emb = (hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "            all_emb.append(emb.float().cpu().numpy())\n",
    "        return np.vstack(all_emb)\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.model, self.tokenizer\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "        cache = os.path.expanduser('~/.cache/huggingface/hub')\n",
    "        if os.path.exists(cache):\n",
    "            for f in os.listdir(cache):\n",
    "                if any(x in f.lower() for x in ['llama','qwen']):\n",
    "                    shutil.rmtree(os.path.join(cache,f), ignore_errors=True)\n",
    "        print(f'  Cleared {self.model_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDCEncoder:\n",
    "    def __init__(self, input_dim, hdc_dim, tau=0.3, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.proj = np.random.randn(input_dim, hdc_dim).astype(np.float32)\n",
    "        self.proj /= np.linalg.norm(self.proj, axis=0, keepdims=True)\n",
    "        self.tau = tau\n",
    "    def encode(self, emb):\n",
    "        p = emb @ self.proj\n",
    "        thr = self.tau * np.std(p, axis=1, keepdims=True)\n",
    "        t = np.zeros_like(p, dtype=np.int8)\n",
    "        t[p > thr] = 1; t[p < -thr] = -1\n",
    "        return t\n",
    "\n",
    "class ContrastiveAligner(nn.Module):\n",
    "    def __init__(self, t_dim, s_dim, h_dim=512):\n",
    "        super().__init__()\n",
    "        self.t_proj = nn.Sequential(nn.Linear(t_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, h_dim))\n",
    "        self.s_proj = nn.Sequential(nn.Linear(s_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, h_dim))\n",
    "    def forward(self, t, s):\n",
    "        return nn.functional.normalize(self.t_proj(t), dim=-1), nn.functional.normalize(self.s_proj(s), dim=-1)\n",
    "\n",
    "def train_aligner(t_emb, s_emb, h_dim, epochs=20, device='cuda'):\n",
    "    h_dim = max(h_dim, min(t_emb.shape[1], s_emb.shape[1]))\n",
    "    print(f'  Aligner: teacher_dim={t_emb.shape[1]}, student_dim={s_emb.shape[1]}, hidden_dim={h_dim}')\n",
    "    aligner = ContrastiveAligner(t_emb.shape[1], s_emb.shape[1], h_dim).to(device)\n",
    "    opt = torch.optim.Adam(aligner.parameters(), lr=1e-3)\n",
    "    T, S = torch.tensor(t_emb, dtype=torch.float32).to(device), torch.tensor(s_emb, dtype=torch.float32).to(device)\n",
    "    bs = min(256, len(T))\n",
    "    for _ in range(epochs):\n",
    "        perm = torch.randperm(len(T))\n",
    "        for i in range(0, len(T), bs):\n",
    "            idx = perm[i:i+bs]\n",
    "            tp, sp = aligner(T[idx], S[idx])\n",
    "            pos = (tp * sp).sum(-1)\n",
    "            neg = (tp * sp[torch.roll(torch.arange(len(idx)), 1)]).sum(-1)\n",
    "            loss = -pos.mean() + torch.clamp(neg + 0.5, min=0).mean()\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "    aligner.eval()\n",
    "    with torch.no_grad(): tp, sp = aligner(T, S); sim = (tp * sp).sum(-1).mean().item()\n",
    "    return aligner, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    ds = load_dataset('glue', 'sst2')['train']\n",
    "    idx = np.random.permutation(len(ds))[:CONFIG['train_size']+CONFIG['test_size']+CONFIG['anchor_size']]\n",
    "    texts = [ds['sentence'][i] for i in idx]\n",
    "    labels = [ds['label'][i] for i in idx]\n",
    "    n1, n2 = CONFIG['train_size'], CONFIG['train_size']+CONFIG['test_size']\n",
    "    return {'train_texts': texts[:n1], 'train_labels': labels[:n1],\n",
    "            'test_texts': texts[n1:n2], 'test_labels': labels[n1:n2],\n",
    "            'anchor_texts': texts[n2:]}\n",
    "\n",
    "data = load_data()\n",
    "print(f'Data: {len(data[\"train_texts\"])} train, {len(data[\"test_texts\"])} test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(teacher_key, student_key, data, seed=42):\n",
    "    np.random.seed(seed); torch.manual_seed(seed)\n",
    "    print(f'\\n--- {teacher_key} → {student_key} (seed={seed}) ---')\n",
    "    result = {'teacher': teacher_key, 'student': student_key, 'seed': seed}\n",
    "    try:\n",
    "        # Teacher\n",
    "        t_ext = EmbeddingExtractor(teacher_key, CONFIG, device)\n",
    "        t_train = t_ext.encode(data['train_texts'])\n",
    "        t_anchor = t_ext.encode(data['anchor_texts'])\n",
    "        result['teacher_dim'] = t_ext.embed_dim\n",
    "        t_ext.clear()\n",
    "        \n",
    "        # Student\n",
    "        s_ext = EmbeddingExtractor(student_key, CONFIG, device)\n",
    "        s_train = s_ext.encode(data['train_texts'])\n",
    "        s_test = s_ext.encode(data['test_texts'])\n",
    "        s_anchor = s_ext.encode(data['anchor_texts'])\n",
    "        s_ext.clear()\n",
    "        \n",
    "        # Ceiling\n",
    "        print('  Computing student ceiling...')\n",
    "        hdc = HDCEncoder(s_train.shape[1], CONFIG['hdc_dim'], CONFIG['tau'], seed)\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "        clf.fit(hdc.encode(s_train), data['train_labels'])\n",
    "        ceiling = clf.score(hdc.encode(s_test), data['test_labels'])\n",
    "        result['student_ceiling'] = ceiling\n",
    "        print(f'  Student ceiling: {ceiling:.1%}')\n",
    "        \n",
    "        # Alignment\n",
    "        print('  Training alignment...')\n",
    "        h_dim = max(768, min(t_train.shape[1], s_train.shape[1]) // 2)\n",
    "        aligner, sim = train_aligner(t_anchor, s_anchor, h_dim, CONFIG['contrastive_epochs'], device)\n",
    "        result['alignment_similarity'] = sim\n",
    "        print(f'  Alignment similarity: {sim:.3f}')\n",
    "        \n",
    "        # Transfer\n",
    "        aligner.eval()\n",
    "        with torch.no_grad():\n",
    "            T = torch.tensor(t_train, dtype=torch.float32).to(device)\n",
    "            S = torch.tensor(s_test, dtype=torch.float32).to(device)\n",
    "            t_aligned = nn.functional.normalize(aligner.t_proj(T), dim=-1).cpu().numpy()\n",
    "            s_aligned = nn.functional.normalize(aligner.s_proj(S), dim=-1).cpu().numpy()\n",
    "        \n",
    "        hdc2 = HDCEncoder(t_aligned.shape[1], CONFIG['hdc_dim'], CONFIG['tau'], seed)\n",
    "        clf2 = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "        clf2.fit(hdc2.encode(t_aligned), data['train_labels'])\n",
    "        acc = clf2.score(hdc2.encode(s_aligned), data['test_labels'])\n",
    "        \n",
    "        result['transfer_accuracy'] = acc\n",
    "        result['transfer_efficiency'] = acc / ceiling\n",
    "        print(f'  Transfer accuracy: {acc:.1%}')\n",
    "        print(f'  Transfer efficiency: {result[\"transfer_efficiency\"]:.1%}')\n",
    "    except Exception as e:\n",
    "        print(f'  ERROR: {e}')\n",
    "        result['error'] = str(e)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('RUNNING EXPERIMENTS')\n",
    "print('=' * 60)\n",
    "\n",
    "all_results = []\n",
    "for t_key, s_key in EXPERIMENTS:\n",
    "    print(f'\\n{\"#\"*60}\\n# {t_key.upper()} → {s_key.upper()}\\n{\"#\"*60}')\n",
    "    for seed in CONFIG['seeds']:\n",
    "        all_results.append(run_experiment(t_key, s_key, data, seed))\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('ALL EXPERIMENTS COMPLETE')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([r for r in all_results if 'error' not in r])\n",
    "\n",
    "print('\\nSUMMARY BY TEACHER:')\n",
    "for t in df['teacher'].unique():\n",
    "    sub = df[df['teacher'] == t]\n",
    "    print(f\"  {t}: acc={sub['transfer_accuracy'].mean():.1%} ± {sub['transfer_accuracy'].std():.1%}, eff={sub['transfer_efficiency'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "def serialize(obj):\n",
    "    if isinstance(obj, dict): return {k: serialize(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list): return [serialize(v) for v in obj]\n",
    "    if isinstance(obj, (np.floating, np.integer)): return float(obj)\n",
    "    return obj\n",
    "\n",
    "with open('paper1_experiment2_results.json', 'w') as f:\n",
    "    json.dump({'config': CONFIG, 'experiments': serialize(all_results)}, f, indent=2)\n",
    "print('Saved to paper1_experiment2_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 1: Cross-Architecture Knowledge Transfer via HDC\n",
    "## Experiment 1: Complete Transfer Pipeline\n",
    "\n",
    "This notebook implements the complete experimental pipeline:\n",
    "- **Stage 0**: Model ceilings (fine-tuned baselines)\n",
    "- **Stage 1**: HDC quantization cost\n",
    "- **Stage 2**: Alignment methods comparison\n",
    "- **Stage 3**: Model pairs (bidirectional transfer)\n",
    "- **Stage 4**: Task generalization (SST-2, AG News)\n",
    "\n",
    "**Author**: Nikolay Yudin  \n",
    "**Project**: Semantic Event Protocol (SEP)  \n",
    "**Repository**: https://github.com/nick-yudin/SEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets torch numpy scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'models': {\n",
    "        'distilbert': 'distilbert-base-uncased',\n",
    "        'gpt2': 'gpt2',\n",
    "        'roberta': 'roberta-base',\n",
    "    },\n",
    "    'datasets': ['sst2', 'ag_news'],  # No MNLI - requires different encoding\n",
    "    'hdc_dims': [1024, 2048, 4096, 8192],\n",
    "    'anchor_sizes': [100, 500, 1000],\n",
    "    'tau': 0.3,  # Ternary threshold\n",
    "    'seeds': [42, 123, 456],\n",
    "    'train_size': 3000,\n",
    "    'test_size': 500,\n",
    "    'anchor_pool_size': 2000,\n",
    "    'classifier_epochs': 20,\n",
    "    'contrastive_epochs': 20,\n",
    "    'finetune_epochs': 3,\n",
    "}\n",
    "\n",
    "print('Configuration loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingExtractor:\n",
    "    \"\"\"Extract embeddings from transformer models using mean pooling.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, device='cuda'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.embed_dim = self.model.config.hidden_size\n",
    "        print(f'Loaded {model_name}: {self.embed_dim}d')\n",
    "    \n",
    "    def encode(self, texts, batch_size=32, show_progress=True):\n",
    "        \"\"\"Extract embeddings using mean pooling.\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(texts), batch_size)\n",
    "        if show_progress:\n",
    "            iterator = tqdm(iterator, desc=f'Encoding')\n",
    "        \n",
    "        for i in iterator:\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch, padding=True, truncation=True,\n",
    "                max_length=128, return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                hidden = outputs.last_hidden_state\n",
    "                mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "                embeddings = (hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Free GPU memory.\"\"\"\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print('EmbeddingExtractor ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDCEncoder:\n",
    "    \"\"\"Encode float embeddings to ternary HDC vectors.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hdc_dim, tau=0.3, seed=42):\n",
    "        self.input_dim = input_dim\n",
    "        self.hdc_dim = hdc_dim\n",
    "        self.tau = tau\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        self.projection = np.random.randn(input_dim, hdc_dim).astype(np.float32)\n",
    "        self.projection /= np.linalg.norm(self.projection, axis=0, keepdims=True)\n",
    "    \n",
    "    def encode(self, embeddings):\n",
    "        \"\"\"Project and quantize to ternary.\"\"\"\n",
    "        projected = embeddings @ self.projection\n",
    "        std = np.std(projected, axis=1, keepdims=True)\n",
    "        threshold = self.tau * std\n",
    "        \n",
    "        ternary = np.zeros_like(projected, dtype=np.int8)\n",
    "        ternary[projected > threshold] = 1\n",
    "        ternary[projected < -threshold] = -1\n",
    "        \n",
    "        return ternary\n",
    "    \n",
    "    def encode_float(self, embeddings):\n",
    "        \"\"\"Project without quantization (for alignment training).\"\"\"\n",
    "        return embeddings @ self.projection\n",
    "\n",
    "print('HDCEncoder ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveAligner(nn.Module):\n",
    "    \"\"\"Learn alignment between teacher and student embedding spaces.\"\"\"\n",
    "    \n",
    "    def __init__(self, teacher_dim, student_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.teacher_proj = nn.Sequential(\n",
    "            nn.Linear(teacher_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.student_proj = nn.Sequential(\n",
    "            nn.Linear(student_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, teacher_emb, student_emb):\n",
    "        t_proj = nn.functional.normalize(self.teacher_proj(teacher_emb), dim=-1)\n",
    "        s_proj = nn.functional.normalize(self.student_proj(student_emb), dim=-1)\n",
    "        return t_proj, s_proj\n",
    "\n",
    "\n",
    "def train_contrastive_aligner(teacher_emb, student_emb, hidden_dim=512, epochs=20, lr=1e-3, device='cuda'):\n",
    "    \"\"\"Train contrastive alignment between embedding spaces.\"\"\"\n",
    "    teacher_dim = teacher_emb.shape[1]\n",
    "    student_dim = student_emb.shape[1]\n",
    "    \n",
    "    aligner = ContrastiveAligner(teacher_dim, student_dim, hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(aligner.parameters(), lr=lr)\n",
    "    \n",
    "    T = torch.tensor(teacher_emb, dtype=torch.float32).to(device)\n",
    "    S = torch.tensor(student_emb, dtype=torch.float32).to(device)\n",
    "    \n",
    "    batch_size = min(256, len(teacher_emb))\n",
    "    n_batches = len(teacher_emb) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(len(teacher_emb))\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            idx = perm[i*batch_size:(i+1)*batch_size]\n",
    "            t_batch = T[idx]\n",
    "            s_batch = S[idx]\n",
    "            \n",
    "            t_proj, s_proj = aligner(t_batch, s_batch)\n",
    "            \n",
    "            # Contrastive loss: positive pairs should be similar\n",
    "            pos_sim = (t_proj * s_proj).sum(dim=-1)\n",
    "            \n",
    "            # Negative pairs: shifted indices\n",
    "            neg_idx = torch.roll(torch.arange(len(idx)), 1)\n",
    "            neg_sim = (t_proj * s_proj[neg_idx]).sum(dim=-1)\n",
    "            \n",
    "            loss = -pos_sim.mean() + torch.clamp(neg_sim + 0.5, min=0).mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Compute final alignment similarity\n",
    "    aligner.eval()\n",
    "    with torch.no_grad():\n",
    "        t_proj, s_proj = aligner(T, S)\n",
    "        similarity = (t_proj * s_proj).sum(dim=-1).mean().item()\n",
    "    \n",
    "    return aligner, similarity\n",
    "\n",
    "print('ContrastiveAligner ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classification_data(dataset_name, train_size, test_size, anchor_size, seed=42):\n",
    "    \"\"\"Load and prepare classification dataset.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if dataset_name == 'sst2':\n",
    "        dataset = load_dataset('glue', 'sst2')\n",
    "        texts = dataset['train']['sentence']\n",
    "        labels = dataset['train']['label']\n",
    "        num_classes = 2\n",
    "    elif dataset_name == 'ag_news':\n",
    "        dataset = load_dataset('ag_news')\n",
    "        texts = dataset['train']['text']\n",
    "        labels = dataset['train']['label']\n",
    "        num_classes = 4\n",
    "    else:\n",
    "        raise ValueError(f'Unknown dataset: {dataset_name}')\n",
    "    \n",
    "    # Shuffle and split\n",
    "    indices = np.random.permutation(len(texts))\n",
    "    total_needed = train_size + test_size + anchor_size\n",
    "    indices = indices[:total_needed]\n",
    "    \n",
    "    texts = [texts[i] for i in indices]\n",
    "    labels = [labels[i] for i in indices]\n",
    "    \n",
    "    return {\n",
    "        'train_texts': texts[:train_size],\n",
    "        'train_labels': labels[:train_size],\n",
    "        'test_texts': texts[train_size:train_size+test_size],\n",
    "        'test_labels': labels[train_size:train_size+test_size],\n",
    "        'anchor_texts': texts[train_size+test_size:],\n",
    "        'num_classes': num_classes,\n",
    "    }\n",
    "\n",
    "print('Data loading ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0: Model Ceilings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_finetuned_ceiling(model_name, dataset_name, seed=42):\n",
    "    \"\"\"Fine-tune model and compute ceiling accuracy.\"\"\"\n",
    "    from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    import evaluate\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Load data\n",
    "    if dataset_name == 'sst2':\n",
    "        dataset = load_dataset('glue', 'sst2')\n",
    "        num_labels = 2\n",
    "        text_key = 'sentence'\n",
    "    else:\n",
    "        dataset = load_dataset('ag_news')\n",
    "        num_labels = 4\n",
    "        text_key = 'text'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=num_labels\n",
    "    )\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    def tokenize(examples):\n",
    "        return tokenizer(examples[text_key], truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    train_data = dataset['train'].shuffle(seed=seed).select(range(3000)).map(tokenize, batched=True)\n",
    "    test_data = dataset['test' if 'test' in dataset else 'validation'].shuffle(seed=seed).select(range(500)).map(tokenize, batched=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./tmp_trainer',\n",
    "        num_train_epochs=CONFIG['finetune_epochs'],\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        logging_steps=100,\n",
    "        save_strategy='no',\n",
    "        report_to='none',\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    preds = trainer.predict(test_data)\n",
    "    accuracy = (preds.predictions.argmax(-1) == preds.label_ids).mean()\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "print('Stage 0 ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 0\n",
    "print('=' * 60)\n",
    "print('STAGE 0: Model Ceilings (Fine-tuned)')\n",
    "print('=' * 60)\n",
    "\n",
    "stage0_results = {}\n",
    "for model_key, model_name in CONFIG['models'].items():\n",
    "    print(f'\\nFine-tuning {model_key}...')\n",
    "    acc = compute_finetuned_ceiling(model_name, 'sst2')\n",
    "    stage0_results[model_key] = acc\n",
    "    print(f'  {model_key}: {acc:.1%}')\n",
    "\n",
    "print('\\nStage 0 complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: HDC Quantization Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hdc_accuracy(embeddings, labels, hdc_dim, tau=0.3, seed=42):\n",
    "    \"\"\"Compute accuracy using HDC ternary vectors.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Split\n",
    "    n_train = int(0.8 * len(embeddings))\n",
    "    train_emb, test_emb = embeddings[:n_train], embeddings[n_train:]\n",
    "    train_labels, test_labels = labels[:n_train], labels[n_train:]\n",
    "    \n",
    "    # HDC encode\n",
    "    hdc = HDCEncoder(embeddings.shape[1], hdc_dim, tau, seed)\n",
    "    train_hdc = hdc.encode(train_emb)\n",
    "    test_hdc = hdc.encode(test_emb)\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    clf.fit(train_hdc, train_labels)\n",
    "    \n",
    "    return clf.score(test_hdc, test_labels)\n",
    "\n",
    "print('Stage 1 functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 1\n",
    "print('=' * 60)\n",
    "print('STAGE 1: HDC Quantization Cost')\n",
    "print('=' * 60)\n",
    "\n",
    "# Load SST-2 data\n",
    "data = load_classification_data('sst2', CONFIG['train_size'], CONFIG['test_size'], 0)\n",
    "all_texts = data['train_texts'] + data['test_texts']\n",
    "all_labels = data['train_labels'] + data['test_labels']\n",
    "\n",
    "stage1_results = {}\n",
    "\n",
    "for model_key, model_name in list(CONFIG['models'].items())[:2]:  # distilbert, gpt2\n",
    "    print(f'\\n{model_key}:')\n",
    "    \n",
    "    extractor = EmbeddingExtractor(model_name, device)\n",
    "    embeddings = extractor.encode(all_texts)\n",
    "    \n",
    "    stage1_results[model_key] = {\n",
    "        'embed_dim': extractor.embed_dim,\n",
    "        'hdc_results': {}\n",
    "    }\n",
    "    \n",
    "    for hdc_dim in CONFIG['hdc_dims']:\n",
    "        acc = compute_hdc_accuracy(embeddings, all_labels, hdc_dim)\n",
    "        stage1_results[model_key]['hdc_results'][hdc_dim] = acc\n",
    "        print(f'  HDC {hdc_dim}d: {acc:.1%}')\n",
    "    \n",
    "    extractor.clear()\n",
    "\n",
    "print('\\nStage 1 complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Alignment Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alignment_experiment(teacher_emb, student_emb, train_labels, test_labels,\n",
    "                            anchor_teacher, anchor_student, method, hdc_dim=4096, seed=42):\n",
    "    \"\"\"Run single alignment experiment.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    result = {'method': method, 'seed': seed}\n",
    "    \n",
    "    # HDC encode\n",
    "    hdc = HDCEncoder(teacher_emb.shape[1], hdc_dim, CONFIG['tau'], seed)\n",
    "    \n",
    "    if method == 'none':\n",
    "        # No alignment - just use same projection\n",
    "        hdc_student = HDCEncoder(student_emb.shape[1], hdc_dim, CONFIG['tau'], seed)\n",
    "        train_hdc = hdc.encode(teacher_emb)\n",
    "        test_hdc = hdc_student.encode(student_emb)\n",
    "        \n",
    "    elif method == 'procrustes':\n",
    "        # Procrustes alignment\n",
    "        R, _ = orthogonal_procrustes(anchor_teacher, anchor_student)\n",
    "        result['align_error'] = np.linalg.norm(anchor_teacher @ R - anchor_student)\n",
    "        \n",
    "        aligned_teacher = teacher_emb @ R\n",
    "        train_hdc = hdc.encode(aligned_teacher)\n",
    "        \n",
    "        hdc_student = HDCEncoder(student_emb.shape[1], hdc_dim, CONFIG['tau'], seed)\n",
    "        test_hdc = hdc_student.encode(student_emb)\n",
    "        \n",
    "    elif method == 'cca':\n",
    "        # CCA alignment\n",
    "        n_components = min(256, anchor_teacher.shape[0] - 1, anchor_teacher.shape[1], anchor_student.shape[1])\n",
    "        cca = CCA(n_components=n_components)\n",
    "        cca.fit(anchor_teacher, anchor_student)\n",
    "        \n",
    "        teacher_cca = cca.transform(teacher_emb)\n",
    "        student_cca = cca.transform(student_emb)\n",
    "        \n",
    "        result['n_components'] = n_components\n",
    "        result['correlation'] = np.corrcoef(teacher_cca[:, 0], student_cca[:, 0])[0, 1]\n",
    "        \n",
    "        hdc_cca = HDCEncoder(n_components, hdc_dim, CONFIG['tau'], seed)\n",
    "        train_hdc = hdc_cca.encode(teacher_cca)\n",
    "        test_hdc = hdc_cca.encode(student_cca)\n",
    "        \n",
    "    elif method == 'contrastive':\n",
    "        # Contrastive alignment\n",
    "        aligner, similarity = train_contrastive_aligner(\n",
    "            anchor_teacher, anchor_student,\n",
    "            hidden_dim=512, epochs=CONFIG['contrastive_epochs'], device=device\n",
    "        )\n",
    "        result['similarity'] = similarity\n",
    "        \n",
    "        # Project through aligner\n",
    "        aligner.eval()\n",
    "        with torch.no_grad():\n",
    "            T = torch.tensor(teacher_emb, dtype=torch.float32).to(device)\n",
    "            S = torch.tensor(student_emb, dtype=torch.float32).to(device)\n",
    "            teacher_proj, _ = aligner(T, T)  # Just need teacher projection\n",
    "            _, student_proj = aligner(S, S)  # Just need student projection\n",
    "            \n",
    "            teacher_aligned = teacher_proj.cpu().numpy()\n",
    "            student_aligned = student_proj.cpu().numpy()\n",
    "        \n",
    "        hdc_aligned = HDCEncoder(teacher_aligned.shape[1], hdc_dim, CONFIG['tau'], seed)\n",
    "        train_hdc = hdc_aligned.encode(teacher_aligned)\n",
    "        test_hdc = hdc_aligned.encode(student_aligned)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    clf.fit(train_hdc, train_labels)\n",
    "    result['accuracy'] = clf.score(test_hdc, test_labels)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print('Stage 2 functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 2\n",
    "print('=' * 60)\n",
    "print('STAGE 2: Alignment Methods Comparison')\n",
    "print('=' * 60)\n",
    "\n",
    "# Load data with anchors\n",
    "data = load_classification_data('sst2', CONFIG['train_size'], CONFIG['test_size'], CONFIG['anchor_pool_size'])\n",
    "\n",
    "# Extract embeddings\n",
    "teacher_name = CONFIG['models']['distilbert']\n",
    "student_name = CONFIG['models']['gpt2']\n",
    "\n",
    "print(f'\\nTeacher: {teacher_name}')\n",
    "teacher_ext = EmbeddingExtractor(teacher_name, device)\n",
    "teacher_train_emb = teacher_ext.encode(data['train_texts'])\n",
    "teacher_anchor_emb = teacher_ext.encode(data['anchor_texts'])\n",
    "teacher_ext.clear()\n",
    "\n",
    "print(f'Student: {student_name}')\n",
    "student_ext = EmbeddingExtractor(student_name, device)\n",
    "student_test_emb = student_ext.encode(data['test_texts'])\n",
    "student_anchor_emb = student_ext.encode(data['anchor_texts'])\n",
    "\n",
    "# Compute student ceiling\n",
    "student_train_emb = student_ext.encode(data['train_texts'])\n",
    "hdc_student = HDCEncoder(student_train_emb.shape[1], 4096, CONFIG['tau'], 42)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(hdc_student.encode(student_train_emb), data['train_labels'])\n",
    "student_ceiling = clf.score(hdc_student.encode(student_test_emb), data['test_labels'])\n",
    "print(f'\\nStudent ceiling: {student_ceiling:.1%}')\n",
    "\n",
    "student_ext.clear()\n",
    "\n",
    "# Run experiments\n",
    "stage2_results = {\n",
    "    'teacher': teacher_name,\n",
    "    'student': student_name,\n",
    "    'student_ceiling': student_ceiling,\n",
    "    'experiments': []\n",
    "}\n",
    "\n",
    "methods = ['none', 'procrustes', 'cca', 'contrastive']\n",
    "\n",
    "for anchor_size in CONFIG['anchor_sizes']:\n",
    "    print(f'\\nAnchor size: {anchor_size}')\n",
    "    anchor_t = teacher_anchor_emb[:anchor_size]\n",
    "    anchor_s = student_anchor_emb[:anchor_size]\n",
    "    \n",
    "    for method in methods:\n",
    "        for seed in CONFIG['seeds']:\n",
    "            result = run_alignment_experiment(\n",
    "                teacher_train_emb, student_test_emb,\n",
    "                data['train_labels'], data['test_labels'],\n",
    "                anchor_t, anchor_s, method, seed=seed\n",
    "            )\n",
    "            result['anchor_size'] = anchor_size\n",
    "            result['efficiency'] = result['accuracy'] / student_ceiling\n",
    "            stage2_results['experiments'].append(result)\n",
    "            print(f'  {method} (seed={seed}): {result[\"accuracy\"]:.1%} ({result[\"efficiency\"]:.1%} eff)')\n",
    "\n",
    "print('\\nStage 2 complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Model Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transfer_experiment(teacher_name, student_name, data, seed=42):\n",
    "    \"\"\"Run transfer from teacher to student.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Extract teacher embeddings\n",
    "    teacher_ext = EmbeddingExtractor(teacher_name, device)\n",
    "    teacher_train_emb = teacher_ext.encode(data['train_texts'])\n",
    "    teacher_anchor_emb = teacher_ext.encode(data['anchor_texts'][:500])\n",
    "    teacher_ext.clear()\n",
    "    \n",
    "    # Extract student embeddings\n",
    "    student_ext = EmbeddingExtractor(student_name, device)\n",
    "    student_test_emb = student_ext.encode(data['test_texts'])\n",
    "    student_anchor_emb = student_ext.encode(data['anchor_texts'][:500])\n",
    "    student_train_emb = student_ext.encode(data['train_texts'])\n",
    "    \n",
    "    # Student ceiling\n",
    "    hdc = HDCEncoder(student_train_emb.shape[1], 4096, CONFIG['tau'], seed)\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    clf.fit(hdc.encode(student_train_emb), data['train_labels'])\n",
    "    ceiling = clf.score(hdc.encode(student_test_emb), data['test_labels'])\n",
    "    \n",
    "    student_ext.clear()\n",
    "    \n",
    "    # Train contrastive aligner\n",
    "    aligner, similarity = train_contrastive_aligner(\n",
    "        teacher_anchor_emb, student_anchor_emb,\n",
    "        hidden_dim=512, epochs=CONFIG['contrastive_epochs'], device=device\n",
    "    )\n",
    "    \n",
    "    # Apply alignment\n",
    "    aligner.eval()\n",
    "    with torch.no_grad():\n",
    "        T = torch.tensor(teacher_train_emb, dtype=torch.float32).to(device)\n",
    "        S = torch.tensor(student_test_emb, dtype=torch.float32).to(device)\n",
    "        teacher_proj = aligner.teacher_proj(T)\n",
    "        student_proj = aligner.student_proj(S)\n",
    "        teacher_aligned = nn.functional.normalize(teacher_proj, dim=-1).cpu().numpy()\n",
    "        student_aligned = nn.functional.normalize(student_proj, dim=-1).cpu().numpy()\n",
    "    \n",
    "    # HDC encode and classify\n",
    "    hdc_aligned = HDCEncoder(teacher_aligned.shape[1], 4096, CONFIG['tau'], seed)\n",
    "    train_hdc = hdc_aligned.encode(teacher_aligned)\n",
    "    test_hdc = hdc_aligned.encode(student_aligned)\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "    clf.fit(train_hdc, data['train_labels'])\n",
    "    accuracy = clf.score(test_hdc, data['test_labels'])\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'accuracy': accuracy,\n",
    "        'efficiency': accuracy / ceiling,\n",
    "        'similarity': similarity,\n",
    "        'ceiling': ceiling\n",
    "    }\n",
    "\n",
    "print('Stage 3 functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 3\n",
    "print('=' * 60)\n",
    "print('STAGE 3: Model Pairs (Bidirectional Transfer)')\n",
    "print('=' * 60)\n",
    "\n",
    "data = load_classification_data('sst2', CONFIG['train_size'], CONFIG['test_size'], CONFIG['anchor_pool_size'])\n",
    "\n",
    "model_pairs = [\n",
    "    ('distilbert', 'gpt2'),\n",
    "    ('gpt2', 'distilbert'),\n",
    "    ('distilbert', 'roberta'),\n",
    "    ('roberta', 'distilbert'),\n",
    "]\n",
    "\n",
    "stage3_results = {'pairs': []}\n",
    "\n",
    "for teacher_key, student_key in model_pairs:\n",
    "    print(f'\\n{teacher_key} → {student_key}')\n",
    "    \n",
    "    teacher_name = CONFIG['models'][teacher_key]\n",
    "    student_name = CONFIG['models'][student_key]\n",
    "    \n",
    "    pair_results = {\n",
    "        'teacher': teacher_key,\n",
    "        'student': student_key,\n",
    "        'transfers': []\n",
    "    }\n",
    "    \n",
    "    for seed in CONFIG['seeds']:\n",
    "        result = run_transfer_experiment(teacher_name, student_name, data, seed)\n",
    "        pair_results['transfers'].append(result)\n",
    "        print(f'  seed={seed}: {result[\"accuracy\"]:.1%} ({result[\"efficiency\"]:.1%} eff)')\n",
    "    \n",
    "    # Aggregate\n",
    "    pair_results['student_ceiling'] = np.mean([r['ceiling'] for r in pair_results['transfers']])\n",
    "    pair_results['mean_accuracy'] = np.mean([r['accuracy'] for r in pair_results['transfers']])\n",
    "    pair_results['mean_efficiency'] = np.mean([r['efficiency'] for r in pair_results['transfers']])\n",
    "    \n",
    "    stage3_results['pairs'].append(pair_results)\n",
    "\n",
    "print('\\nStage 3 complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Task Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 4\n",
    "print('=' * 60)\n",
    "print('STAGE 4: Task Generalization (SST-2, AG News)')\n",
    "print('=' * 60)\n",
    "\n",
    "stage4_results = {'datasets': {}}\n",
    "\n",
    "for dataset_name in CONFIG['datasets']:\n",
    "    print(f'\\n--- {dataset_name.upper()} ---')\n",
    "    \n",
    "    data = load_classification_data(dataset_name, CONFIG['train_size'], CONFIG['test_size'], CONFIG['anchor_pool_size'])\n",
    "    \n",
    "    dataset_results = {\n",
    "        'name': dataset_name,\n",
    "        'num_classes': data['num_classes'],\n",
    "        'pairs': []\n",
    "    }\n",
    "    \n",
    "    for teacher_key, student_key in [('distilbert', 'gpt2'), ('gpt2', 'distilbert')]:\n",
    "        print(f'\\n{teacher_key} → {student_key}')\n",
    "        \n",
    "        teacher_name = CONFIG['models'][teacher_key]\n",
    "        student_name = CONFIG['models'][student_key]\n",
    "        \n",
    "        pair_results = {\n",
    "            'teacher': teacher_key,\n",
    "            'student': student_key,\n",
    "            'transfers': []\n",
    "        }\n",
    "        \n",
    "        for seed in CONFIG['seeds']:\n",
    "            result = run_transfer_experiment(teacher_name, student_name, data, seed)\n",
    "            pair_results['transfers'].append({\n",
    "                'seed': seed,\n",
    "                'accuracy': result['accuracy'],\n",
    "                'efficiency': result['efficiency']\n",
    "            })\n",
    "            print(f'  seed={seed}: {result[\"accuracy\"]:.1%} ({result[\"efficiency\"]:.1%} eff)')\n",
    "        \n",
    "        pair_results['student_ceiling'] = np.mean([r['ceiling'] for r in [result]])\n",
    "        pair_results['mean_accuracy'] = np.mean([r['accuracy'] for r in pair_results['transfers']])\n",
    "        pair_results['mean_efficiency'] = np.mean([r['efficiency'] for r in pair_results['transfers']])\n",
    "        \n",
    "        dataset_results['pairs'].append(pair_results)\n",
    "    \n",
    "    stage4_results['datasets'][dataset_name] = dataset_results\n",
    "\n",
    "print('\\nStage 4 complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.float32, np.float64, np.floating)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    return obj\n",
    "\n",
    "# Compile all results\n",
    "all_results = {\n",
    "    'config': CONFIG,\n",
    "    'stage0': stage0_results,\n",
    "    'stage1': stage1_results,\n",
    "    'stage2': stage2_results,\n",
    "    'stage3': stage3_results,\n",
    "    'stage4': stage4_results,\n",
    "}\n",
    "\n",
    "# Save\n",
    "with open('paper1_experiment1_results.json', 'w') as f:\n",
    "    json.dump(convert_to_serializable(all_results), f, indent=2)\n",
    "\n",
    "print('Results saved to paper1_experiment1_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Stage 1: HDC Dimension\n",
    "ax = axes[0, 0]\n",
    "dims = CONFIG['hdc_dims']\n",
    "for model_key in ['distilbert', 'gpt2']:\n",
    "    accs = [stage1_results[model_key]['hdc_results'][d] for d in dims]\n",
    "    ax.plot(dims, accs, 'o-', label=model_key, markersize=8)\n",
    "ax.axhline(stage0_results['distilbert'], ls='--', alpha=0.5, label='distilbert ceiling')\n",
    "ax.axhline(stage0_results['gpt2'], ls='--', alpha=0.5, label='gpt2 ceiling')\n",
    "ax.set_xlabel('HDC Dimension')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Stage 1: HDC Dimension Effect')\n",
    "ax.legend()\n",
    "ax.set_ylim(0.7, 0.95)\n",
    "\n",
    "# Stage 2: Alignment Methods\n",
    "ax = axes[0, 1]\n",
    "methods = ['none', 'procrustes', 'cca', 'contrastive']\n",
    "colors = ['gray', 'orange', 'green', 'red']\n",
    "for method, color in zip(methods, colors):\n",
    "    means, stds = [], []\n",
    "    for anchor_size in CONFIG['anchor_sizes']:\n",
    "        accs = [e['accuracy'] for e in stage2_results['experiments'] \n",
    "               if e['method'] == method and e['anchor_size'] == anchor_size]\n",
    "        means.append(np.mean(accs))\n",
    "        stds.append(np.std(accs))\n",
    "    ax.errorbar(CONFIG['anchor_sizes'], means, yerr=stds, marker='o', \n",
    "               label=method, color=color, capsize=4)\n",
    "ax.axhline(stage2_results['student_ceiling'], ls='--', color='green', alpha=0.5, label='Student ceiling')\n",
    "ax.set_xlabel('Anchor Size')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Stage 2: Alignment Methods')\n",
    "ax.legend()\n",
    "ax.set_ylim(0.4, 0.85)\n",
    "\n",
    "# Stage 3: Model Pairs\n",
    "ax = axes[1, 0]\n",
    "labels = [f\"{p['teacher']}→{p['student']}\" for p in stage3_results['pairs']]\n",
    "ceilings = [p['student_ceiling'] for p in stage3_results['pairs']]\n",
    "transfers = [p['mean_accuracy'] for p in stage3_results['pairs']]\n",
    "x = np.arange(len(labels))\n",
    "ax.bar(x - 0.2, ceilings, 0.35, label='Student ceiling', alpha=0.7)\n",
    "ax.bar(x + 0.2, transfers, 0.35, label='Transfer', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=15)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Stage 3: Model Pairs')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Stage 4: Datasets\n",
    "ax = axes[1, 1]\n",
    "datasets = list(stage4_results['datasets'].keys())\n",
    "pair1_accs = [stage4_results['datasets'][d]['pairs'][0]['mean_accuracy'] for d in datasets]\n",
    "pair2_accs = [stage4_results['datasets'][d]['pairs'][1]['mean_accuracy'] for d in datasets]\n",
    "x = np.arange(len(datasets))\n",
    "ax.bar(x - 0.2, pair1_accs, 0.35, label='distilbert→gpt2', alpha=0.7)\n",
    "ax.bar(x + 0.2, pair2_accs, 0.35, label='gpt2→distilbert', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([d.upper() for d in datasets])\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Stage 4: Different Datasets')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('paper1_experiment1_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Visualization saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('EXPERIMENT 1 SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\nStage 0 - Model Ceilings:')\n",
    "for model, acc in stage0_results.items():\n",
    "    print(f'  {model}: {acc:.1%}')\n",
    "\n",
    "print('\\nStage 1 - HDC Cost (8192d):')\n",
    "for model in ['distilbert', 'gpt2']:\n",
    "    ceiling = stage0_results[model]\n",
    "    hdc = stage1_results[model]['hdc_results'][8192]\n",
    "    print(f'  {model}: {ceiling:.1%} → {hdc:.1%} (loss: {(hdc-ceiling)*100:.1f}%)')\n",
    "\n",
    "print('\\nStage 2 - Alignment Methods (500 anchors, mean):')\n",
    "for method in ['none', 'procrustes', 'cca', 'contrastive']:\n",
    "    accs = [e['accuracy'] for e in stage2_results['experiments'] \n",
    "           if e['method'] == method and e['anchor_size'] == 500]\n",
    "    print(f'  {method}: {np.mean(accs):.1%}')\n",
    "\n",
    "print('\\nStage 3 - Model Pairs:')\n",
    "for p in stage3_results['pairs']:\n",
    "    print(f\"  {p['teacher']}→{p['student']}: {p['mean_accuracy']:.1%} ({p['mean_efficiency']:.1%} eff)\")\n",
    "\n",
    "print('\\nStage 4 - Datasets:')\n",
    "for ds_name, ds_data in stage4_results['datasets'].items():\n",
    "    acc = np.mean([p['mean_accuracy'] for p in ds_data['pairs']])\n",
    "    eff = np.mean([p['mean_efficiency'] for p in ds_data['pairs']])\n",
    "    print(f'  {ds_name}: {acc:.1%} ({eff:.1%} eff)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {"cell_type": "markdown", "source": ["# ğŸŒ M4c: Cross-Lingual Semantic Transfer\n\n## The Big Claim\n**\"Meaning is universal. Train on English, test on any language.\"**\n\n## For Paper: \"...Until We Found Meaning\"\nThis experiment proves the title is justified."], "metadata": {"id": "header"}},
    {"cell_type": "code", "execution_count": null, "metadata": {"id": "install"}, "outputs": [], "source": ["!pip install -q sentence-transformers datasets\nprint(\"Done\")"]},
    {"cell_type": "code", "source": ["import torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport json\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")"], "metadata": {"id": "imports"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["ENCODER_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\nencoder = SentenceTransformer(ENCODER_NAME)\nSEMANTIC_DIM = encoder.get_sentence_embedding_dimension()\nprint(f\"Encoder: {SEMANTIC_DIM}d\")"], "metadata": {"id": "encoder"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["sst2 = load_dataset('glue', 'sst2')\nTRAIN_SIZE = 3000\ntrain_data = sst2['train'].shuffle(seed=42).select(range(TRAIN_SIZE))\ntest_en = sst2['validation']\nprint(f\"English: train {len(train_data)}, test {len(test_en)}\")"], "metadata": {"id": "load_en"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["multilingual_tests = {}\nlang_names = {'en': 'English', 'de': 'German', 'fr': 'French', 'es': 'Spanish', 'ja': 'Japanese', 'zh': 'Chinese'}\n\nfor lang in ['de', 'fr', 'es', 'ja', 'zh']:\n    try:\n        ds = load_dataset('amazon_reviews_multi', lang, split='test')\n        ds_filtered = ds.filter(lambda x: x['stars'] != 3)\n        ds_subset = ds_filtered.shuffle(seed=42).select(range(min(500, len(ds_filtered))))\n        multilingual_tests[lang] = {\n            'texts': [x['review_body'] for x in ds_subset],\n            'labels': np.array([1 if x['stars'] >= 4 else 0 for x in ds_subset])}\n        print(f\"{lang_names[lang]}: {len(ds_subset)}\")\n    except Exception as e:\n        print(f\"{lang}: {e}\")"], "metadata": {"id": "load_multi"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["train_texts = list(train_data['sentence'])\ntrain_labels = np.array(train_data['label'])\ntest_en_texts = list(test_en['sentence'])\ntest_en_labels = np.array(test_en['label'])\n\ntrain_emb = encoder.encode(train_texts, show_progress_bar=True)\ntest_en_emb = encoder.encode(test_en_texts, show_progress_bar=True)\n\nmultilingual_emb = {}\nfor lang, data in multilingual_tests.items():\n    multilingual_emb[lang] = encoder.encode(data['texts'], show_progress_bar=False)\n    print(f\"{lang}: done\")"], "metadata": {"id": "encode"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["HDC_DIM = 4096\nnp.random.seed(42)\nprojection = np.random.randn(SEMANTIC_DIM, HDC_DIM).astype(np.float32)\nprojection /= np.linalg.norm(projection, axis=0, keepdims=True)\n\ndef to_hdc(emb):\n    hdc = emb @ projection\n    thr = 0.3 * np.std(hdc, axis=1, keepdims=True)\n    return np.where(hdc > thr, 1, np.where(hdc < -thr, -1, 0)).astype(np.float32)\n\ntrain_hdc = to_hdc(train_emb)\ntest_en_hdc = to_hdc(test_en_emb)\nmultilingual_hdc = {lang: to_hdc(emb) for lang, emb in multilingual_emb.items()}\nprint(\"HDC done\")"], "metadata": {"id": "hdc"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["class HDCDataset(Dataset):\n    def __init__(self, hdc, labels):\n        self.hdc = torch.tensor(hdc, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n    def __len__(self): return len(self.labels)\n    def __getitem__(self, idx): return self.hdc[idx], self.labels[idx]\n\nclass HDCClassifier(nn.Module):\n    def __init__(self, hdc_dim=4096):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(hdc_dim, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, 2))\n    def forward(self, x): return self.net(x)"], "metadata": {"id": "model"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["train_loader = DataLoader(HDCDataset(train_hdc, train_labels), batch_size=32, shuffle=True)\nmodel = HDCClassifier(HDC_DIM).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(20):\n    model.train()\n    for hdc, labels in train_loader:\n        hdc, labels = hdc.to(device), labels.to(device)\n        optimizer.zero_grad()\n        criterion(model(hdc), labels).backward()\n        optimizer.step()\n    scheduler.step()\n    if (epoch+1) % 5 == 0: print(f\"Epoch {epoch+1}\")\nprint(\"Training done\")"], "metadata": {"id": "train"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["def evaluate(hdc, labels):\n    model.eval()\n    with torch.no_grad():\n        preds = torch.argmax(model(torch.tensor(hdc).to(device)), dim=1).cpu().numpy()\n    return accuracy_score(labels, preds)\n\nresults = {}\nacc_en = evaluate(test_en_hdc, test_en_labels)\nresults['en'] = acc_en\nprint(f\"English: {acc_en:.1%}\")\n\nfor lang, hdc in multilingual_hdc.items():\n    acc = evaluate(hdc, multilingual_tests[lang]['labels'])\n    results[lang] = acc\n    print(f\"{lang_names[lang]}: {acc:.1%}\")\n\ncross_avg = np.mean([results[l] for l in multilingual_hdc.keys()])\nprint(f\"\\nCross-lingual avg: {cross_avg:.1%}\")\nprint(f\"Transfer ratio: {cross_avg/acc_en:.1%}\")"], "metadata": {"id": "eval"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["fig, ax = plt.subplots(figsize=(10, 6))\nlanguages = ['en'] + list(multilingual_hdc.keys())\naccs = [results[l] for l in languages]\nflags = ['EN', 'DE', 'FR', 'ES', 'JA', 'ZH']\ncolors = ['green'] + ['lightblue']*5\nbars = ax.bar(range(len(languages)), accs, color=colors, edgecolor='black')\nax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\nax.set_xticks(range(len(languages)))\nax.set_xticklabels([f\"{flags[i]}\\n{lang_names[l]}\" for i, l in enumerate(languages)])\nax.set_ylabel('Accuracy')\nax.set_title('Cross-Lingual Transfer: Train English, Test All\\n\"...Until We Found Meaning\"')\nax.set_ylim(0.4, 1.0)\nfor bar, acc in zip(bars, accs):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{acc:.1%}', ha='center', fontweight='bold')\nplt.tight_layout()\nplt.savefig('m4c_crosslingual_results.png', dpi=150)\nplt.show()"], "metadata": {"id": "viz"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["output = {'experiment': 'M4c Cross-Lingual', 'paper': '...Until We Found Meaning',\n    'encoder': ENCODER_NAME, 'hdc_dim': HDC_DIM, 'train_lang': 'English', 'train_size': TRAIN_SIZE,\n    'results': {lang_names.get(k,k): float(v) for k,v in results.items()},\n    'english_acc': float(acc_en), 'crosslingual_avg': float(cross_avg),\n    'transfer_ratio': float(cross_avg/acc_en), 'timestamp': datetime.now().isoformat()}\nwith open('m4c_crosslingual_results.json', 'w') as f: json.dump(output, f, indent=2)\nprint(json.dumps(output, indent=2))"], "metadata": {"id": "save"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["if cross_avg > 0.75:\n    print(\"\\nâœ… STRONG transfer! HDC captures MEANING.\")\n    print(\"   Title '...Until We Found Meaning' is JUSTIFIED.\")\nelif cross_avg > 0.65:\n    print(\"\\nğŸ“ˆ Good transfer. Semantic features generalize.\")\nelse:\n    print(\"\\nâš ï¸ Limited transfer.\")\nprint(f\"\\nKey: Train EN â†’ {cross_avg:.1%} on 5 languages\")"], "metadata": {"id": "conclusion"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["from google.colab import files\nfiles.download('m4c_crosslingual_results.json')\nfiles.download('m4c_crosslingual_results.png')"], "metadata": {"id": "download"}, "execution_count": null, "outputs": []}
  ]
}

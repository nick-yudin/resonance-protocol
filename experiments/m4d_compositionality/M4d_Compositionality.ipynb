{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {"colab": {"provenance": [], "gpuType": "T4"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "accelerator": "GPU"},
  "cells": [
    {"cell_type": "markdown", "source": ["# M4d: Compositionality — Semantic Arithmetic\n\n```\nking - man + woman = queen\nparis - france + germany = berlin\n```\n\nIf HDC preserves these → we captured MEANING."], "metadata": {"id": "h"}},
    {"cell_type": "code", "execution_count": null, "metadata": {"id": "i"}, "outputs": [], "source": ["!pip install -q sentence-transformers\nprint('Done')"]},
    {"cell_type": "code", "source": ["import numpy as np\nimport matplotlib.pyplot as plt\nimport json\nfrom datetime import datetime\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nnp.random.seed(42)"], "metadata": {"id": "imp"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["encoder = SentenceTransformer('all-mpnet-base-v2')\nSEMANTIC_DIM = encoder.get_sentence_embedding_dimension()\nprint(f'Encoder: {SEMANTIC_DIM}d')"], "metadata": {"id": "enc"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["HDC_DIM = 4096\nproj = np.random.randn(SEMANTIC_DIM, HDC_DIM).astype(np.float32)\nproj /= np.linalg.norm(proj, axis=0, keepdims=True)\n\ndef to_hdc(text):\n    emb = encoder.encode(text)\n    hdc = emb @ proj\n    thr = 0.3 * np.std(hdc)\n    return np.where(hdc > thr, 1, np.where(hdc < -thr, -1, 0)).astype(np.float32)\n\ndef to_float_hdc(text):\n    return encoder.encode(text) @ proj\n\nprint('HDC ready')"], "metadata": {"id": "hdc"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["ANALOGIES = [\n    ('king', 'man', 'woman', 'queen'),\n    ('paris', 'france', 'germany', 'berlin'),\n    ('tokyo', 'japan', 'france', 'paris'),\n    ('madrid', 'spain', 'italy', 'rome'),\n    ('walked', 'walk', 'swim', 'swam'),\n    ('running', 'run', 'walk', 'walking'),\n    ('bigger', 'big', 'small', 'smaller'),\n    ('doctor', 'hospital', 'teacher', 'school'),\n    ('ceo', 'company', 'captain', 'team'),\n    ('puppy', 'dog', 'kitten', 'cat'),\n    ('hot', 'cold', 'tall', 'short'),\n    ('happy', 'sad', 'fast', 'slow'),\n]\nprint(f'{len(ANALOGIES)} analogies')"], "metadata": {"id": "analogies"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["VOCAB = list(set([w for a in ANALOGIES for w in a]))\nVOCAB += ['prince','princess','london','moscow','beijing','washington','ran','jumped',\n          'largest','smallest','nurse','university','president','player','baby','adult',\n          'warm','cool','angry','quick','boy','girl','husband','wife','brother','sister']\nVOCAB = list(set(VOCAB))\nprint(f'Vocabulary: {len(VOCAB)} words')"], "metadata": {"id": "vocab"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["print('Encoding vocabulary...')\nvocab_emb = {w: encoder.encode(w) for w in VOCAB}\nvocab_hdc = {w: to_hdc(w) for w in VOCAB}\nvocab_hdc_float = {w: to_float_hdc(w) for w in VOCAB}\nprint(f'Done: {len(VOCAB)} words')"], "metadata": {"id": "encode"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["def find_nearest(query_vec, vocab_dict, exclude=None, top_k=5):\n    exclude = exclude or []\n    sims = [(w, cosine_similarity([query_vec], [v])[0][0]) for w, v in vocab_dict.items() if w not in exclude]\n    sims.sort(key=lambda x: x[1], reverse=True)\n    return sims[:top_k]\n\nprint('Nearest to king:', find_nearest(vocab_hdc_float['king'], vocab_hdc_float, ['king'])[:3])"], "metadata": {"id": "nearest"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["def test_analogy(a, b, c, expected, vocab_dict, requantize=False):\n    result_vec = vocab_dict[a] - vocab_dict[b] + vocab_dict[c]\n    if requantize:\n        thr = 0.3 * np.std(result_vec)\n        result_vec = np.where(result_vec > thr, 1, np.where(result_vec < -thr, -1, 0))\n    nearest = find_nearest(result_vec, vocab_dict, exclude=[a,b,c], top_k=5)\n    top_words = [w for w,s in nearest]\n    return {'analogy': f'{a} - {b} + {c}', 'expected': expected, 'got': nearest[0][0],\n            'top5': top_words, 'hit1': nearest[0][0] == expected, 'hit5': expected in top_words}"], "metadata": {"id": "test_fn"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["print('='*60)\nprint('BASELINE: Original Embeddings (768d)')\nprint('='*60)\nbase_results = [test_analogy(a,b,c,d,vocab_emb) for a,b,c,d in ANALOGIES]\nfor r in base_results:\n    s = '✓' if r['hit1'] else ('~' if r['hit5'] else '✗')\n    print(f\"{s} {r['analogy']} = {r['got']} (expected: {r['expected']})\")\nbase_top1 = np.mean([r['hit1'] for r in base_results])\nbase_top5 = np.mean([r['hit5'] for r in base_results])\nprint(f'\\nBaseline Top-1: {base_top1:.1%}, Top-5: {base_top5:.1%}')"], "metadata": {"id": "baseline"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["print('\\n' + '='*60)\nprint('FLOAT HDC (4096d)')\nprint('='*60)\nfloat_results = [test_analogy(a,b,c,d,vocab_hdc_float) for a,b,c,d in ANALOGIES]\nfor r in float_results:\n    s = '✓' if r['hit1'] else ('~' if r['hit5'] else '✗')\n    print(f\"{s} {r['analogy']} = {r['got']} (expected: {r['expected']})\")\nfloat_top1 = np.mean([r['hit1'] for r in float_results])\nfloat_top5 = np.mean([r['hit5'] for r in float_results])\nprint(f'\\nFloat HDC Top-1: {float_top1:.1%}, Top-5: {float_top5:.1%}')"], "metadata": {"id": "float_hdc"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["print('\\n' + '='*60)\nprint('TERNARY HDC (4096d)')\nprint('='*60)\ntern_results = [test_analogy(a,b,c,d,vocab_hdc,requantize=True) for a,b,c,d in ANALOGIES]\nfor r in tern_results:\n    s = '✓' if r['hit1'] else ('~' if r['hit5'] else '✗')\n    print(f\"{s} {r['analogy']} = {r['got']} (expected: {r['expected']})\")\ntern_top1 = np.mean([r['hit1'] for r in tern_results])\ntern_top5 = np.mean([r['hit5'] for r in tern_results])\nprint(f'\\nTernary HDC Top-1: {tern_top1:.1%}, Top-5: {tern_top5:.1%}')"], "metadata": {"id": "ternary"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["print('\\n' + '='*60)\nprint('SUMMARY')\nprint('='*60)\nprint(f'{\"Method\":<30} {\"Top-1\":>10} {\"Top-5\":>10} {\"vs Base\":>10}')\nprint('-'*60)\nprint(f'{\"Original (768d float)\":<30} {base_top1:>10.1%} {base_top5:>10.1%} {\"baseline\":>10}')\nprint(f'{\"Float HDC (4096d)\":<30} {float_top1:>10.1%} {float_top5:>10.1%} {float_top5/base_top5 if base_top5>0 else 0:>10.1%}')\nprint(f'{\"Ternary HDC (4096d)\":<30} {tern_top1:>10.1%} {tern_top5:>10.1%} {tern_top5/base_top5 if base_top5>0 else 0:>10.1%}')\nprint('\\n✓ = exact match, ~ = in top-5, ✗ = miss')"], "metadata": {"id": "summary"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nax = axes[0]\nmethods = ['Original\\n(768d)', 'Float HDC\\n(4096d)', 'Ternary\\n(4096d)']\nx = np.arange(3)\nw = 0.35\nbars1 = ax.bar(x - w/2, [base_top1, float_top1, tern_top1], w, label='Top-1', color='steelblue')\nbars2 = ax.bar(x + w/2, [base_top5, float_top5, tern_top5], w, label='Top-5', color='lightblue')\nax.set_ylabel('Accuracy')\nax.set_title('Semantic Arithmetic Preservation')\nax.set_xticks(x)\nax.set_xticklabels(methods)\nax.legend()\nax.set_ylim(0, 1.1)\nfor b in list(bars1)+list(bars2): ax.text(b.get_x()+b.get_width()/2, b.get_height()+0.02, f'{b.get_height():.0%}', ha='center', fontsize=9)\n\nax = axes[1]\nhits = np.array([[r['hit1'] for r in base_results], [r['hit1'] for r in float_results], [r['hit1'] for r in tern_results]])\nim = ax.imshow(hits, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\nax.set_yticks([0,1,2])\nax.set_yticklabels(['Original', 'Float HDC', 'Ternary'])\nax.set_xticks(range(len(ANALOGIES)))\nax.set_xticklabels([a[0][:4] for a in ANALOGIES], rotation=45, ha='right')\nax.set_title('Per-Analogy (green=correct)')\n\nplt.tight_layout()\nplt.savefig('m4d_compositionality.png', dpi=150)\nplt.show()"], "metadata": {"id": "viz"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["output = {'experiment': 'M4d Compositionality', 'paper': '...Until We Found Meaning',\n          'num_analogies': len(ANALOGIES), 'vocab_size': len(VOCAB),\n          'results': {'baseline': {'top1': float(base_top1), 'top5': float(base_top5)},\n                      'float_hdc': {'top1': float(float_top1), 'top5': float(float_top5)},\n                      'ternary_hdc': {'top1': float(tern_top1), 'top5': float(tern_top5)}},\n          'retention_rate': float(tern_top5/base_top5) if base_top5 > 0 else 0,\n          'timestamp': datetime.now().isoformat()}\nwith open('m4d_compositionality.json', 'w') as f: json.dump(output, f, indent=2)\nprint(json.dumps(output, indent=2))"], "metadata": {"id": "save"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["print('\\n' + '='*60)\nif tern_top5/base_top5 > 0.8 if base_top5 > 0 else tern_top5 > 0.5:\n    print('✅ SUCCESS! HDC preserves semantic arithmetic')\n    print('   \"king - man + woman = queen\" works in ternary!')\nelse:\n    print('⚠️ Partial preservation')\nprint('='*60)"], "metadata": {"id": "conclusion"}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["from google.colab import files\nfiles.download('m4d_compositionality.json')\nfiles.download('m4d_compositionality.png')"], "metadata": {"id": "dl"}, "execution_count": null, "outputs": []}
  ]
}

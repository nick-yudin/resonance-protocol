{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ M2.5e: Curriculum Learning Optimization\n",
        "\n",
        "**Observation from M2.5d:** Curriculum reached loss 1.1615 at step 7, then jumped back to 1.2633.\n",
        "\n",
        "**Hypothesis:** We can capture that optimal point by:\n",
        "1. **Checkpointing** ‚Äî save model at each step, pick the best\n",
        "2. **LR Decay** ‚Äî reduce learning rate when switching to hard examples\n",
        "3. **Gradual Curriculum** ‚Äî smoother transition from easy to hard\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "!pip install -q sentence-transformers scikit-learn\n",
        "!pip install -q tqdm numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Prepare Data (Same as M2.5d)"
      ],
      "metadata": {
        "id": "data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "print(\"Loading Alpaca dataset...\")\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "\n",
        "POOL_SIZE = 2000\n",
        "SUBSET_SIZE = 500\n",
        "N_CLUSTERS = 500\n",
        "\n",
        "random.seed(42)\n",
        "pool_indices = random.sample(range(len(dataset)), POOL_SIZE)\n",
        "pool = dataset.select(pool_indices)\n",
        "\n",
        "def format_example(example):\n",
        "    text = f\"Instruction: {example['instruction']}\"\n",
        "    if example.get('input'):\n",
        "        text += f\"\\nInput: {example['input']}\"\n",
        "    text += f\"\\nOutput: {example['output']}\"\n",
        "    return text\n",
        "\n",
        "pool_texts = [format_example(ex) for ex in pool]\n",
        "\n",
        "# Encode\n",
        "print(\"Encoding with SentenceTransformer...\")\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "st_embeddings = st_model.encode(pool_texts, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HDC Encoder\n",
        "class TernaryHDCEncoder:\n",
        "    def __init__(self, input_dim=384, hd_dim=10000, sparsity=0.7, seed=42):\n",
        "        self.hd_dim = hd_dim\n",
        "        self.sparsity = sparsity\n",
        "        np.random.seed(seed)\n",
        "        self.projection = np.random.randn(input_dim, hd_dim).astype(np.float32)\n",
        "        self.projection /= np.sqrt(input_dim)\n",
        "    \n",
        "    def encode(self, embeddings):\n",
        "        projected = embeddings @ self.projection\n",
        "        ternary = np.zeros_like(projected)\n",
        "        for i in range(len(projected)):\n",
        "            vec = projected[i]\n",
        "            threshold = np.percentile(np.abs(vec), self.sparsity * 100)\n",
        "            ternary[i] = np.where(vec > threshold, 1,\n",
        "                                   np.where(vec < -threshold, -1, 0))\n",
        "        return ternary\n",
        "\n",
        "print(\"Encoding with HDC...\")\n",
        "hdc_encoder = TernaryHDCEncoder()\n",
        "hdc_embeddings = hdc_encoder.encode(st_embeddings)\n",
        "\n",
        "# Cluster\n",
        "print(\"Clustering...\")\n",
        "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(hdc_embeddings)\n",
        "\n",
        "# Calculate distances\n",
        "distances_to_centroids = pairwise_distances(hdc_embeddings, kmeans.cluster_centers_)\n",
        "point_distances = np.array([distances_to_centroids[i, cluster_labels[i]] \n",
        "                            for i in range(len(hdc_embeddings))])\n",
        "\n",
        "print(f\"Distance range: {point_distances.min():.2f} - {point_distances.max():.2f}\")"
      ],
      "metadata": {
        "id": "encode_cluster"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select centroid and boundary samples\n",
        "def select_from_clusters(strategy='centroid'):\n",
        "    selected = []\n",
        "    for cluster_id in range(N_CLUSTERS):\n",
        "        cluster_mask = cluster_labels == cluster_id\n",
        "        cluster_indices = np.where(cluster_mask)[0]\n",
        "        if len(cluster_indices) == 0:\n",
        "            continue\n",
        "        cluster_distances = point_distances[cluster_indices]\n",
        "        if strategy == 'centroid':\n",
        "            best_idx = cluster_indices[np.argmin(cluster_distances)]\n",
        "        else:  # boundary\n",
        "            best_idx = cluster_indices[np.argmax(cluster_distances)]\n",
        "        selected.append(best_idx)\n",
        "    return list(set(selected))[:SUBSET_SIZE]\n",
        "\n",
        "centroid_indices = select_from_clusters('centroid')\n",
        "boundary_indices = select_from_clusters('boundary')\n",
        "\n",
        "print(f\"Centroid samples: {len(centroid_indices)}\")\n",
        "print(f\"Boundary samples: {len(boundary_indices)}\")"
      ],
      "metadata": {
        "id": "select_samples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create Curriculum Datasets\n",
        "\n",
        "We'll create several curriculum variants:\n",
        "1. **Sharp Curriculum** ‚Äî 250 easy, then 250 hard (original)\n",
        "2. **Gradual Curriculum** ‚Äî sorted by difficulty (easiest ‚Üí hardest)\n",
        "3. **3-Phase Curriculum** ‚Äî easy ‚Üí medium ‚Üí hard"
      ],
      "metadata": {
        "id": "curriculum_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all samples sorted by distance to centroid (difficulty)\n",
        "# Lower distance = easier (more typical), Higher distance = harder (more unusual)\n",
        "\n",
        "all_indices = list(range(POOL_SIZE))\n",
        "sorted_by_difficulty = sorted(all_indices, key=lambda i: point_distances[i])\n",
        "\n",
        "# Take top 500 (mix of easy and medium)\n",
        "gradual_indices = sorted_by_difficulty[:SUBSET_SIZE]\n",
        "\n",
        "# Verify difficulty distribution\n",
        "gradual_distances = point_distances[gradual_indices]\n",
        "print(f\"Gradual curriculum difficulty range: {gradual_distances.min():.3f} - {gradual_distances.max():.3f}\")\n",
        "print(f\"Mean difficulty: {gradual_distances.mean():.3f}\")"
      ],
      "metadata": {
        "id": "gradual_curriculum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "\n",
        "# 1. Sharp Curriculum (original M2.5d): 250 centroid + 250 boundary\n",
        "n_half = SUBSET_SIZE // 2\n",
        "sharp_curriculum_indices = centroid_indices[:n_half] + boundary_indices[:n_half]\n",
        "sharp_curriculum = pool.select(sharp_curriculum_indices)\n",
        "\n",
        "# 2. Gradual Curriculum: sorted by difficulty\n",
        "gradual_curriculum = pool.select(gradual_indices)\n",
        "\n",
        "# 3. 3-Phase Curriculum: 200 easy + 150 medium + 150 hard\n",
        "n_easy = 200\n",
        "n_medium = 150\n",
        "n_hard = 150\n",
        "\n",
        "easy_indices = sorted_by_difficulty[:n_easy]\n",
        "medium_indices = sorted_by_difficulty[POOL_SIZE//3 : POOL_SIZE//3 + n_medium]\n",
        "hard_indices = sorted_by_difficulty[-n_hard:]\n",
        "\n",
        "three_phase_indices = easy_indices + medium_indices + hard_indices\n",
        "three_phase_curriculum = pool.select(three_phase_indices)\n",
        "\n",
        "print(f\"Sharp curriculum: {len(sharp_curriculum)} samples\")\n",
        "print(f\"Gradual curriculum: {len(gradual_curriculum)} samples\")\n",
        "print(f\"3-Phase curriculum: {len(three_phase_curriculum)} samples\")"
      ],
      "metadata": {
        "id": "create_curriculums"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Load Model"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainerCallback\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_function(examples):\n",
        "    texts = []\n",
        "    for i in range(len(examples['instruction'])):\n",
        "        text = f\"### Instruction:\\n{examples['instruction'][i]}\\n\\n\"\n",
        "        if examples['input'][i]:\n",
        "            text += f\"### Input:\\n{examples['input'][i]}\\n\\n\"\n",
        "        text += f\"### Response:\\n{examples['output'][i]}\"\n",
        "        texts.append(text)\n",
        "    \n",
        "    tokenized = tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing...\")\n",
        "sharp_tokenized = sharp_curriculum.map(tokenize_function, batched=True, remove_columns=sharp_curriculum.column_names)\n",
        "gradual_tokenized = gradual_curriculum.map(tokenize_function, batched=True, remove_columns=gradual_curriculum.column_names)\n",
        "three_phase_tokenized = three_phase_curriculum.map(tokenize_function, batched=True, remove_columns=three_phase_curriculum.column_names)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "tokenize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Custom Trainer with Checkpointing"
      ],
      "metadata": {
        "id": "trainer_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom callback to track all losses and find minimum\n",
        "class LossTrackingCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.losses = []\n",
        "        self.steps = []\n",
        "        self.best_loss = float('inf')\n",
        "        self.best_step = 0\n",
        "    \n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and 'loss' in logs:\n",
        "            loss = logs['loss']\n",
        "            step = state.global_step\n",
        "            self.losses.append(loss)\n",
        "            self.steps.append(step)\n",
        "            \n",
        "            if loss < self.best_loss:\n",
        "                self.best_loss = loss\n",
        "                self.best_step = step\n",
        "                print(f\"  üìç New best loss: {loss:.4f} at step {step}\")"
      ],
      "metadata": {
        "id": "callback"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_tracking(train_dataset, run_name, use_lr_decay=False):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {run_name}\")\n",
        "    if use_lr_decay:\n",
        "        print(\"Using LR decay (cosine schedule)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    model = get_peft_model(base_model, lora_config)\n",
        "    \n",
        "    # LR schedule: constant vs cosine decay\n",
        "    lr_scheduler_type = \"cosine\" if use_lr_decay else \"constant\"\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{run_name}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=lr_scheduler_type,\n",
        "        warmup_steps=20,\n",
        "        logging_steps=5,  # More frequent logging to catch the minimum\n",
        "        save_strategy=\"no\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "    \n",
        "    # Add our tracking callback\n",
        "    tracker = LossTrackingCallback()\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[tracker]\n",
        "    )\n",
        "    \n",
        "    trainer.train()\n",
        "    \n",
        "    final_loss = tracker.losses[-1] if tracker.losses else float('inf')\n",
        "    \n",
        "    print(f\"\\nüìä Summary:\")\n",
        "    print(f\"  Final loss: {final_loss:.4f}\")\n",
        "    print(f\"  Best loss: {tracker.best_loss:.4f} at step {tracker.best_step}\")\n",
        "    print(f\"  Potential improvement: {((final_loss - tracker.best_loss) / final_loss) * 100:.1f}%\")\n",
        "    \n",
        "    result = {\n",
        "        'name': run_name,\n",
        "        'final_loss': final_loss,\n",
        "        'best_loss': tracker.best_loss,\n",
        "        'best_step': tracker.best_step,\n",
        "        'all_losses': tracker.losses,\n",
        "        'all_steps': tracker.steps,\n",
        "        'lr_decay': use_lr_decay\n",
        "    }\n",
        "    \n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return result"
      ],
      "metadata": {
        "id": "train_function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Run Experiments"
      ],
      "metadata": {
        "id": "experiments_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "# 1. Sharp Curriculum (baseline from M2.5d)\n",
        "results['sharp'] = train_with_tracking(sharp_tokenized, 'sharp_curriculum', use_lr_decay=False)\n",
        "\n",
        "# 2. Sharp Curriculum + LR Decay\n",
        "results['sharp_lr_decay'] = train_with_tracking(sharp_tokenized, 'sharp_lr_decay', use_lr_decay=True)\n",
        "\n",
        "# 3. Gradual Curriculum (sorted by difficulty)\n",
        "results['gradual'] = train_with_tracking(gradual_tokenized, 'gradual_curriculum', use_lr_decay=False)\n",
        "\n",
        "# 4. Gradual + LR Decay\n",
        "results['gradual_lr_decay'] = train_with_tracking(gradual_tokenized, 'gradual_lr_decay', use_lr_decay=True)\n",
        "\n",
        "# 5. 3-Phase Curriculum\n",
        "results['three_phase'] = train_with_tracking(three_phase_tokenized, 'three_phase', use_lr_decay=False)\n",
        "\n",
        "# 6. 3-Phase + LR Decay\n",
        "results['three_phase_lr_decay'] = train_with_tracking(three_phase_tokenized, 'three_phase_lr_decay', use_lr_decay=True)"
      ],
      "metadata": {
        "id": "run_experiments"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Results"
      ],
      "metadata": {
        "id": "results_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot all loss curves\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "colors = {\n",
        "    'sharp': 'blue',\n",
        "    'sharp_lr_decay': 'lightblue',\n",
        "    'gradual': 'green',\n",
        "    'gradual_lr_decay': 'lightgreen',\n",
        "    'three_phase': 'red',\n",
        "    'three_phase_lr_decay': 'salmon'\n",
        "}\n",
        "\n",
        "for idx, (name, data) in enumerate(results.items()):\n",
        "    ax = axes[idx]\n",
        "    ax.plot(data['all_steps'], data['all_losses'], color=colors[name], linewidth=2)\n",
        "    ax.axhline(y=data['best_loss'], color='gold', linestyle='--', alpha=0.7)\n",
        "    ax.scatter([data['best_step']], [data['best_loss']], color='gold', s=100, zorder=5, marker='*')\n",
        "    ax.set_title(f\"{name}\\nFinal: {data['final_loss']:.4f}, Best: {data['best_loss']:.4f}\")\n",
        "    ax.set_xlabel('Step')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('m2.5e_all_experiments.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plot_individual"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for name, data in results.items():\n",
        "    plt.plot(data['all_losses'], label=f\"{name} (best: {data['best_loss']:.4f})\", \n",
        "             color=colors[name], linewidth=2)\n",
        "\n",
        "# Reference lines\n",
        "plt.axhline(y=1.2194, color='black', linestyle='--', alpha=0.5, label='M2.5c Best (1.2194)')\n",
        "plt.axhline(y=1.1615, color='gold', linestyle='--', alpha=0.5, label='M2.5d Observed Min (1.1615)')\n",
        "\n",
        "plt.xlabel('Logging Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('M2.5e: Curriculum Learning Optimization\\nFinding the Optimal Point')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('m2.5e_combined.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plot_combined"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä M2.5e RESULTS: CURRICULUM OPTIMIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "PREV_BEST = 1.2194  # M2.5c\n",
        "OBSERVED_MIN = 1.1615  # From M2.5d graph\n",
        "\n",
        "print(f\"\\n{'Strategy':<25} {'Final Loss':>12} {'Best Loss':>12} {'Best Step':>10} {'vs M2.5c':>12}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "# Sort by best_loss\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['best_loss'])\n",
        "\n",
        "for name, data in sorted_results:\n",
        "    vs_prev = ((PREV_BEST - data['best_loss']) / PREV_BEST) * 100\n",
        "    status = \"üèÜ\" if data['best_loss'] < PREV_BEST else \"\"\n",
        "    print(f\"{name:<25} {data['final_loss']:>12.4f} {data['best_loss']:>12.4f} {data['best_step']:>10} {vs_prev:>+11.2f}% {status}\")\n",
        "\n",
        "print(f\"\\nReference points:\")\n",
        "print(f\"  M2.5c HDC-Curated: {PREV_BEST}\")\n",
        "print(f\"  M2.5d Observed minimum: {OBSERVED_MIN}\")\n",
        "\n",
        "# Find overall best\n",
        "best_name, best_data = sorted_results[0]\n",
        "print(f\"\\nüèÜ OVERALL BEST: {best_name}\")\n",
        "print(f\"   Best loss: {best_data['best_loss']:.4f} at step {best_data['best_step']}\")\n",
        "print(f\"   Improvement over M2.5c: {((PREV_BEST - best_data['best_loss']) / PREV_BEST) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key insight analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî¨ KEY INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compare LR decay effect\n",
        "print(\"\\n1. Effect of LR Decay:\")\n",
        "for base in ['sharp', 'gradual', 'three_phase']:\n",
        "    no_decay = results[base]['best_loss']\n",
        "    with_decay = results[f'{base}_lr_decay']['best_loss']\n",
        "    diff = ((no_decay - with_decay) / no_decay) * 100\n",
        "    better = \"‚úÖ LR decay helps\" if with_decay < no_decay else \"‚ùå LR decay hurts\"\n",
        "    print(f\"   {base}: {no_decay:.4f} ‚Üí {with_decay:.4f} ({diff:+.2f}%) {better}\")\n",
        "\n",
        "# Compare curriculum types\n",
        "print(\"\\n2. Best curriculum type:\")\n",
        "curriculum_types = ['sharp', 'gradual', 'three_phase']\n",
        "for ct in curriculum_types:\n",
        "    best_of_type = min(results[ct]['best_loss'], results[f'{ct}_lr_decay']['best_loss'])\n",
        "    print(f\"   {ct}: {best_of_type:.4f}\")\n",
        "\n",
        "# Early stopping potential\n",
        "print(\"\\n3. Early Stopping Potential (Final vs Best):\")\n",
        "for name, data in sorted_results:\n",
        "    potential = ((data['final_loss'] - data['best_loss']) / data['final_loss']) * 100\n",
        "    if potential > 1:\n",
        "        print(f\"   {name}: Could save {potential:.1f}% by stopping at step {data['best_step']}\")"
      ],
      "metadata": {
        "id": "insights"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "import json\n",
        "\n",
        "output = {\n",
        "    \"phase\": \"M2.5e\",\n",
        "    \"experiment\": \"Curriculum Learning Optimization\",\n",
        "    \"goal\": \"Capture the optimal point observed in M2.5d (1.1615)\",\n",
        "    \"previous_best\": PREV_BEST,\n",
        "    \"observed_minimum_m2.5d\": OBSERVED_MIN,\n",
        "    \"results\": {\n",
        "        name: {\n",
        "            \"final_loss\": float(data['final_loss']),\n",
        "            \"best_loss\": float(data['best_loss']),\n",
        "            \"best_step\": int(data['best_step']),\n",
        "            \"lr_decay\": data['lr_decay'],\n",
        "            \"vs_m2.5c_pct\": float(((PREV_BEST - data['best_loss']) / PREV_BEST) * 100)\n",
        "        }\n",
        "        for name, data in results.items()\n",
        "    },\n",
        "    \"overall_best\": {\n",
        "        \"strategy\": best_name,\n",
        "        \"best_loss\": float(best_data['best_loss']),\n",
        "        \"best_step\": int(best_data['best_step']),\n",
        "        \"improvement_over_m2.5c_pct\": float(((PREV_BEST - best_data['best_loss']) / PREV_BEST) * 100)\n",
        "    },\n",
        "    \"beat_m2.5c\": best_data['best_loss'] < PREV_BEST,\n",
        "    \"beat_observed_minimum\": best_data['best_loss'] < OBSERVED_MIN\n",
        "}\n",
        "\n",
        "with open('phase_m2.5e_results.json', 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"\\nüìÅ Results saved to phase_m2.5e_results.json\")\n",
        "print(\"\\n\" + json.dumps(output, indent=2))"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "from google.colab import files\n",
        "files.download('phase_m2.5e_results.json')\n",
        "files.download('m2.5e_combined.png')\n",
        "files.download('m2.5e_all_experiments.png')"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

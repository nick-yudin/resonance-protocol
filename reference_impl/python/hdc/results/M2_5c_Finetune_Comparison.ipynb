{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Resonance Protocol ‚Äî Phase M2.5c: Fine-tuning Comparison\n",
        "\n",
        "**Goal:** Compare model quality after fine-tuning on:\n",
        "- Random subset (500 samples)\n",
        "- SentenceTransformer-curated subset (500 samples)\n",
        "- HDC-curated subset (500 samples)\n",
        "\n",
        "**Success Criteria:**\n",
        "- HDC-curated Loss ‚â§ ST-curated Loss ‚Üí HDC is valid replacement\n",
        "- HDC-curated Loss < Random Loss ‚Üí curation works\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup Environment"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "!pip install -q sentence-transformers scikit-learn\n",
        "!pip install -q tqdm numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load Dataset and Create Subsets"
      ],
      "metadata": {
        "id": "data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load Alpaca dataset\n",
        "print(\"Loading Alpaca dataset...\")\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "\n",
        "# Take subset for speed (2000 samples to curate from)\n",
        "POOL_SIZE = 2000\n",
        "SUBSET_SIZE = 500\n",
        "\n",
        "random.seed(42)\n",
        "pool_indices = random.sample(range(len(dataset)), POOL_SIZE)\n",
        "pool = dataset.select(pool_indices)\n",
        "print(f\"Pool size: {len(pool)}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text representations for encoding\n",
        "def format_example(example):\n",
        "    \"\"\"Format instruction + input + output as single text\"\"\"\n",
        "    text = f\"Instruction: {example['instruction']}\"\n",
        "    if example.get('input'):\n",
        "        text += f\"\\nInput: {example['input']}\"\n",
        "    text += f\"\\nOutput: {example['output']}\"\n",
        "    return text\n",
        "\n",
        "pool_texts = [format_example(ex) for ex in pool]\n",
        "print(f\"Example text:\\n{pool_texts[0][:500]}...\")"
      ],
      "metadata": {
        "id": "format_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode with SentenceTransformer\n",
        "print(\"\\nEncoding with SentenceTransformer...\")\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "st_embeddings = st_model.encode(pool_texts, show_progress_bar=True)\n",
        "print(f\"ST embeddings shape: {st_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "encode_st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HDC Encoder (Projection + Ternary)\n",
        "class TernaryHDCEncoder:\n",
        "    def __init__(self, input_dim=384, hd_dim=10000, sparsity=0.7, seed=42):\n",
        "        self.input_dim = input_dim\n",
        "        self.hd_dim = hd_dim\n",
        "        self.sparsity = sparsity\n",
        "        \n",
        "        # Fixed random projection matrix\n",
        "        np.random.seed(seed)\n",
        "        self.projection = np.random.randn(input_dim, hd_dim).astype(np.float32)\n",
        "        self.projection /= np.sqrt(input_dim)  # Normalize\n",
        "    \n",
        "    def encode(self, embeddings):\n",
        "        \"\"\"Project to HD space and ternarize\"\"\"\n",
        "        # Project\n",
        "        projected = embeddings @ self.projection\n",
        "        \n",
        "        # Ternarize: keep top/bottom (1-sparsity), zero middle\n",
        "        ternary = np.zeros_like(projected)\n",
        "        for i in range(len(projected)):\n",
        "            vec = projected[i]\n",
        "            threshold = np.percentile(np.abs(vec), self.sparsity * 100)\n",
        "            ternary[i] = np.where(vec > threshold, 1,\n",
        "                                   np.where(vec < -threshold, -1, 0))\n",
        "        return ternary\n",
        "\n",
        "# Encode with HDC\n",
        "print(\"\\nEncoding with HDC...\")\n",
        "hdc_encoder = TernaryHDCEncoder(input_dim=384, hd_dim=10000, sparsity=0.7)\n",
        "hdc_embeddings = hdc_encoder.encode(st_embeddings)\n",
        "print(f\"HDC embeddings shape: {hdc_embeddings.shape}\")\n",
        "print(f\"Sparsity: {(hdc_embeddings == 0).mean():.1%}\")"
      ],
      "metadata": {
        "id": "encode_hdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 3 subsets\n",
        "\n",
        "# 1. Random subset\n",
        "random.seed(123)\n",
        "random_indices = random.sample(range(POOL_SIZE), SUBSET_SIZE)\n",
        "print(f\"Random subset: {len(random_indices)} samples\")\n",
        "\n",
        "# 2. ST-curated (K-means clustering)\n",
        "print(\"\\nClustering ST embeddings...\")\n",
        "st_kmeans = KMeans(n_clusters=SUBSET_SIZE, random_state=42, n_init=10)\n",
        "st_kmeans.fit(st_embeddings)\n",
        "\n",
        "# Find nearest sample to each centroid\n",
        "from sklearn.metrics import pairwise_distances\n",
        "st_distances = pairwise_distances(st_kmeans.cluster_centers_, st_embeddings)\n",
        "st_curated_indices = [int(np.argmin(st_distances[i])) for i in range(SUBSET_SIZE)]\n",
        "st_curated_indices = list(set(st_curated_indices))  # Remove duplicates\n",
        "print(f\"ST-curated subset: {len(st_curated_indices)} samples\")\n",
        "\n",
        "# 3. HDC-curated (K-means clustering)\n",
        "print(\"\\nClustering HDC embeddings...\")\n",
        "hdc_kmeans = KMeans(n_clusters=SUBSET_SIZE, random_state=42, n_init=10)\n",
        "hdc_kmeans.fit(hdc_embeddings)\n",
        "\n",
        "hdc_distances = pairwise_distances(hdc_kmeans.cluster_centers_, hdc_embeddings)\n",
        "hdc_curated_indices = [int(np.argmin(hdc_distances[i])) for i in range(SUBSET_SIZE)]\n",
        "hdc_curated_indices = list(set(hdc_curated_indices))  # Remove duplicates\n",
        "print(f\"HDC-curated subset: {len(hdc_curated_indices)} samples\")"
      ],
      "metadata": {
        "id": "create_subsets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets for fine-tuning\n",
        "random_subset = pool.select(random_indices)\n",
        "st_subset = pool.select(st_curated_indices)\n",
        "hdc_subset = pool.select(hdc_curated_indices)\n",
        "\n",
        "print(f\"Random subset size: {len(random_subset)}\")\n",
        "print(f\"ST-curated subset size: {len(st_subset)}\")\n",
        "print(f\"HDC-curated subset size: {len(hdc_subset)}\")"
      ],
      "metadata": {
        "id": "create_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Fine-tuning Setup"
      ],
      "metadata": {
        "id": "finetune_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Load model and tokenizer\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load in 4-bit for memory efficiency\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "print(\"LoRA config ready\")"
      ],
      "metadata": {
        "id": "lora_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    texts = []\n",
        "    for i in range(len(examples['instruction'])):\n",
        "        text = f\"### Instruction:\\n{examples['instruction'][i]}\\n\\n\"\n",
        "        if examples['input'][i]:\n",
        "            text += f\"### Input:\\n{examples['input'][i]}\\n\\n\"\n",
        "        text += f\"### Response:\\n{examples['output'][i]}\"\n",
        "        texts.append(text)\n",
        "    \n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize all subsets\n",
        "print(\"Tokenizing datasets...\")\n",
        "random_tokenized = random_subset.map(tokenize_function, batched=True, remove_columns=random_subset.column_names)\n",
        "st_tokenized = st_subset.map(tokenize_function, batched=True, remove_columns=st_subset.column_names)\n",
        "hdc_tokenized = hdc_subset.map(tokenize_function, batched=True, remove_columns=hdc_subset.column_names)\n",
        "print(\"Tokenization complete!\")"
      ],
      "metadata": {
        "id": "tokenize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training Function"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(train_dataset, run_name):\n",
        "    \"\"\"Train model and return loss history\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {run_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Create fresh PEFT model\n",
        "    model = get_peft_model(base_model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{run_name}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        warmup_steps=50,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"no\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    train_result = trainer.train()\n",
        "    \n",
        "    # Get loss history\n",
        "    loss_history = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
        "    \n",
        "    print(f\"\\nFinal loss: {loss_history[-1]:.4f}\")\n",
        "    \n",
        "    # Clean up to free memory\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return {\n",
        "        'name': run_name,\n",
        "        'final_loss': loss_history[-1],\n",
        "        'loss_history': loss_history,\n",
        "        'train_samples': len(train_dataset)\n",
        "    }"
      ],
      "metadata": {
        "id": "train_function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Run Experiments"
      ],
      "metadata": {
        "id": "experiments_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on all three subsets\n",
        "results = {}\n",
        "\n",
        "# 1. Random baseline\n",
        "results['random'] = train_and_evaluate(random_tokenized, 'random')\n",
        "\n",
        "# 2. ST-curated\n",
        "results['st_curated'] = train_and_evaluate(st_tokenized, 'st_curated')\n",
        "\n",
        "# 3. HDC-curated\n",
        "results['hdc_curated'] = train_and_evaluate(hdc_tokenized, 'hdc_curated')"
      ],
      "metadata": {
        "id": "run_experiments"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Results & Visualization"
      ],
      "metadata": {
        "id": "results_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for name, data in results.items():\n",
        "    plt.plot(data['loss_history'], label=f\"{name} (final: {data['final_loss']:.4f})\")\n",
        "\n",
        "plt.xlabel('Training Steps (√ó10)')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Phase M2.5c: Fine-tuning Comparison\\nRandom vs ST-Curated vs HDC-Curated')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('m2.5c_loss_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "plot_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary table\n",
        "print(\"\\nüìä PHASE M2.5c RESULTS\\n\")\n",
        "print(f\"{'Method':<20} {'Samples':>10} {'Final Loss':>12} {'Status':>10}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "random_loss = results['random']['final_loss']\n",
        "st_loss = results['st_curated']['final_loss']\n",
        "hdc_loss = results['hdc_curated']['final_loss']\n",
        "\n",
        "# Determine winner\n",
        "losses = {'Random': random_loss, 'ST-Curated': st_loss, 'HDC-Curated': hdc_loss}\n",
        "winner = min(losses, key=losses.get)\n",
        "\n",
        "for name, data in results.items():\n",
        "    status = \"üëë BEST\" if data['final_loss'] == min(random_loss, st_loss, hdc_loss) else \"\"\n",
        "    print(f\"{name:<20} {data['train_samples']:>10} {data['final_loss']:>12.4f} {status:>10}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "\n",
        "# Analysis\n",
        "print(\"\\nüî¨ ANALYSIS\\n\")\n",
        "\n",
        "hdc_vs_random = ((random_loss - hdc_loss) / random_loss) * 100\n",
        "hdc_vs_st = ((st_loss - hdc_loss) / st_loss) * 100\n",
        "st_vs_random = ((random_loss - st_loss) / random_loss) * 100\n",
        "\n",
        "print(f\"HDC vs Random: {hdc_vs_random:+.2f}% {'‚úÖ HDC better' if hdc_vs_random > 0 else '‚ùå Random better'}\")\n",
        "print(f\"HDC vs ST:     {hdc_vs_st:+.2f}% {'‚úÖ HDC better' if hdc_vs_st > 0 else '‚ö†Ô∏è ST better'}\")\n",
        "print(f\"ST vs Random:  {st_vs_random:+.2f}% {'‚úÖ ST better' if st_vs_random > 0 else '‚ùå Random better'}\")\n",
        "\n",
        "# Verdict\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"\\nüìã VERDICT\\n\")\n",
        "\n",
        "if hdc_loss <= st_loss and hdc_loss < random_loss:\n",
        "    print(\"‚úÖ SUCCESS: HDC-curated ‚â§ ST-curated < Random\")\n",
        "    print(\"   HDC is a valid replacement for ST in data curation!\")\n",
        "    verdict = \"SUCCESS\"\n",
        "elif hdc_loss < random_loss:\n",
        "    print(\"‚ö†Ô∏è PARTIAL SUCCESS: HDC-curated < Random, but ST is better\")\n",
        "    print(\"   HDC curation works, but doesn't beat ST embeddings.\")\n",
        "    verdict = \"PARTIAL\"\n",
        "else:\n",
        "    print(\"‚ùå FAILURE: Random ‚â• HDC-curated\")\n",
        "    print(\"   Curation didn't help in this experiment.\")\n",
        "    verdict = \"FAILURE\"\n",
        "\n",
        "print(f\"\\nWinner: {winner}\")"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results as JSON for RESEARCH_LOG\n",
        "import json\n",
        "\n",
        "output = {\n",
        "    \"phase\": \"M2.5c\",\n",
        "    \"experiment\": \"Fine-tuning Comparison\",\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"pool_size\": POOL_SIZE,\n",
        "    \"subset_size\": SUBSET_SIZE,\n",
        "    \"results\": {\n",
        "        \"random\": {\n",
        "            \"final_loss\": float(random_loss),\n",
        "            \"samples\": results['random']['train_samples']\n",
        "        },\n",
        "        \"st_curated\": {\n",
        "            \"final_loss\": float(st_loss),\n",
        "            \"samples\": results['st_curated']['train_samples']\n",
        "        },\n",
        "        \"hdc_curated\": {\n",
        "            \"final_loss\": float(hdc_loss),\n",
        "            \"samples\": results['hdc_curated']['train_samples']\n",
        "        }\n",
        "    },\n",
        "    \"comparison\": {\n",
        "        \"hdc_vs_random_pct\": float(hdc_vs_random),\n",
        "        \"hdc_vs_st_pct\": float(hdc_vs_st),\n",
        "        \"st_vs_random_pct\": float(st_vs_random)\n",
        "    },\n",
        "    \"winner\": winner,\n",
        "    \"verdict\": verdict\n",
        "}\n",
        "\n",
        "with open('phase_m2.5c_results.json', 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"\\nüìÅ Results saved to phase_m2.5c_results.json\")\n",
        "print(\"\\nCopy this JSON to your RESEARCH_LOG.md:\")\n",
        "print(json.dumps(output, indent=2))"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Download Results\n",
        "\n",
        "Download:\n",
        "- `phase_m2.5c_results.json` ‚Äî raw results\n",
        "- `m2.5c_loss_curves.png` ‚Äî visualization"
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download results\n",
        "files.download('phase_m2.5c_results.json')\n",
        "files.download('m2.5c_loss_curves.png')"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

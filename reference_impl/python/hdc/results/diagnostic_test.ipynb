{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üîç Diagnostic: Why 0% Accuracy with Near-Zero Loss?\n",
        "\n",
        "Something is wrong. Let's debug step by step."
      ],
      "metadata": {"id": "intro"}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "setup"},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MINIMAL TEST DATA\n",
        "# ============================================================\n",
        "\n",
        "# Super simple examples\n",
        "EXAMPLES = [\n",
        "    ('walk', 'WALK'),\n",
        "    ('run', 'RUN'),\n",
        "    ('jump', 'JUMP'),\n",
        "    ('walk twice', 'WALK WALK'),\n",
        "    ('run twice', 'RUN RUN'),\n",
        "]\n",
        "\n",
        "print(\"Training examples:\")\n",
        "for cmd, out in EXAMPLES:\n",
        "    print(f\"  '{cmd}' ‚Üí '{out}'\")"
      ],
      "metadata": {"id": "data"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# VOCABULARY\n",
        "# ============================================================\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "        self.n_words = 4\n",
        "    \n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split():\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.n_words\n",
        "                self.idx2word[self.n_words] = word\n",
        "                self.n_words += 1\n",
        "    \n",
        "    def encode(self, sentence, add_sos=False, add_eos=True):\n",
        "        tokens = []\n",
        "        if add_sos:\n",
        "            tokens.append(self.word2idx['<SOS>'])\n",
        "        tokens.extend([self.word2idx.get(w, self.word2idx['<UNK>']) for w in sentence.split()])\n",
        "        if add_eos:\n",
        "            tokens.append(self.word2idx['<EOS>'])\n",
        "        return tokens\n",
        "    \n",
        "    def decode(self, indices):\n",
        "        words = []\n",
        "        for idx in indices:\n",
        "            if isinstance(idx, torch.Tensor):\n",
        "                idx = idx.item()\n",
        "            if idx == self.word2idx['<EOS>']:\n",
        "                break\n",
        "            if idx not in [self.word2idx['<PAD>'], self.word2idx['<SOS>']]:\n",
        "                words.append(self.idx2word.get(idx, '<UNK>'))\n",
        "        return ' '.join(words)\n",
        "\n",
        "# Build vocabularies\n",
        "src_vocab = Vocabulary()\n",
        "tgt_vocab = Vocabulary()\n",
        "\n",
        "for cmd, out in EXAMPLES:\n",
        "    src_vocab.add_sentence(cmd.lower())\n",
        "    tgt_vocab.add_sentence(out)\n",
        "\n",
        "print(f\"Source vocab ({src_vocab.n_words}): {src_vocab.word2idx}\")\n",
        "print(f\"Target vocab ({tgt_vocab.n_words}): {tgt_vocab.word2idx}\")"
      ],
      "metadata": {"id": "vocab"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TEST ENCODING/DECODING\n",
        "# ============================================================\n",
        "\n",
        "print(\"Testing encode/decode:\")\n",
        "for cmd, out in EXAMPLES:\n",
        "    src_enc = src_vocab.encode(cmd.lower())\n",
        "    tgt_enc = tgt_vocab.encode(out)\n",
        "    \n",
        "    src_dec = src_vocab.decode(src_enc)\n",
        "    tgt_dec = tgt_vocab.decode(tgt_enc)\n",
        "    \n",
        "    print(f\"  '{cmd}' ‚Üí {src_enc} ‚Üí '{src_dec}'\")\n",
        "    print(f\"  '{out}' ‚Üí {tgt_enc} ‚Üí '{tgt_dec}'\")\n",
        "    print(f\"  Match: {tgt_dec == out}\")\n",
        "    print()"
      ],
      "metadata": {"id": "test_vocab"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DATASET\n",
        "# ============================================================\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, examples, src_vocab, tgt_vocab, max_len=10):\n",
        "        self.examples = examples\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        cmd, out = self.examples[idx]\n",
        "        \n",
        "        src = self.src_vocab.encode(cmd.lower())\n",
        "        tgt = self.tgt_vocab.encode(out, add_sos=True, add_eos=True)  # Add SOS for teacher forcing\n",
        "        \n",
        "        # Pad\n",
        "        src = src[:self.max_len] + [0] * max(0, self.max_len - len(src))\n",
        "        tgt = tgt[:self.max_len] + [0] * max(0, self.max_len - len(tgt))\n",
        "        \n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "dataset = SimpleDataset(EXAMPLES, src_vocab, tgt_vocab)\n",
        "loader = DataLoader(dataset, batch_size=5, shuffle=True)\n",
        "\n",
        "# Check one batch\n",
        "src_batch, tgt_batch = next(iter(loader))\n",
        "print(f\"Source batch shape: {src_batch.shape}\")\n",
        "print(f\"Target batch shape: {tgt_batch.shape}\")\n",
        "print(f\"\\nFirst source: {src_batch[0].tolist()}\")\n",
        "print(f\"First target: {tgt_batch[0].tolist()}\")\n",
        "print(f\"Decoded target: '{tgt_vocab.decode(tgt_batch[0])}'\")"
      ],
      "metadata": {"id": "dataset"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SIMPLE TRANSFORMER\n",
        "# ============================================================\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, nhead=4, num_layers=2, max_len=10):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "        \n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        \n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=d_model*4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "    \n",
        "    def forward(self, src, tgt):\n",
        "        B = src.size(0)\n",
        "        src_len = src.size(1)\n",
        "        tgt_len = tgt.size(1)\n",
        "        \n",
        "        # Positions\n",
        "        src_pos = torch.arange(src_len, device=src.device).unsqueeze(0).expand(B, -1)\n",
        "        tgt_pos = torch.arange(tgt_len, device=tgt.device).unsqueeze(0).expand(B, -1)\n",
        "        \n",
        "        # Embeddings\n",
        "        src_emb = self.src_emb(src) + self.pos_emb(src_pos)\n",
        "        tgt_emb = self.tgt_emb(tgt) + self.pos_emb(tgt_pos)\n",
        "        \n",
        "        # Masks\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len, device=src.device)\n",
        "        src_pad_mask = (src == 0)\n",
        "        tgt_pad_mask = (tgt == 0)\n",
        "        \n",
        "        # Forward\n",
        "        out = self.transformer(\n",
        "            src_emb, tgt_emb,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_pad_mask,\n",
        "            tgt_key_padding_mask=tgt_pad_mask\n",
        "        )\n",
        "        \n",
        "        return self.fc_out(out)\n",
        "    \n",
        "    def generate(self, src, max_len=10, verbose=False):\n",
        "        \"\"\"Autoregressive generation.\"\"\"\n",
        "        self.eval()\n",
        "        B = src.size(0)\n",
        "        \n",
        "        # Start with SOS token\n",
        "        tgt = torch.ones(B, 1, dtype=torch.long, device=src.device)  # SOS = 1\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"  Starting generation with SOS token\")\n",
        "        \n",
        "        for step in range(max_len):\n",
        "            with torch.no_grad():\n",
        "                output = self.forward(src, tgt)\n",
        "                logits = output[:, -1, :]  # Last position\n",
        "                next_token = logits.argmax(dim=-1, keepdim=True)\n",
        "                \n",
        "                if verbose:\n",
        "                    probs = torch.softmax(logits, dim=-1)\n",
        "                    top_prob, top_idx = probs[0].topk(3)\n",
        "                    print(f\"  Step {step}: predicted token {next_token[0].item()}, \"\n",
        "                          f\"top3: {[(idx.item(), f'{p:.2f}') for idx, p in zip(top_idx, top_prob)]}\")\n",
        "                \n",
        "                tgt = torch.cat([tgt, next_token], dim=1)\n",
        "                \n",
        "                # Stop if EOS\n",
        "                if (next_token == 2).all():  # EOS = 2\n",
        "                    break\n",
        "        \n",
        "        return tgt\n",
        "\n",
        "model = SimpleTransformer(\n",
        "    src_vocab_size=src_vocab.n_words,\n",
        "    tgt_vocab_size=tgt_vocab.n_words,\n",
        "    d_model=64,\n",
        "    nhead=4,\n",
        "    num_layers=2\n",
        ").to(device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {n_params:,}\")"
      ],
      "metadata": {"id": "model"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TRAINING\n",
        "# ============================================================\n",
        "\n",
        "def train(model, loader, epochs=200):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore PAD\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        \n",
        "        for src, tgt in loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            \n",
        "            # Teacher forcing: input is tgt[:-1], target is tgt[1:]\n",
        "            tgt_input = tgt[:, :-1]  # Remove last token\n",
        "            tgt_output = tgt[:, 1:]  # Remove first token (SOS)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(src, tgt_input)\n",
        "            \n",
        "            # Reshape for loss\n",
        "            output = output.reshape(-1, output.size(-1))\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "            \n",
        "            loss = criterion(output, tgt_output)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Loss = {total_loss/len(loader):.6f}\")\n",
        "    \n",
        "    return total_loss / len(loader)\n",
        "\n",
        "final_loss = train(model, loader, epochs=200)\n",
        "print(f\"\\nFinal loss: {final_loss:.6f}\")"
      ],
      "metadata": {"id": "train"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DETAILED EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DETAILED EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for cmd, expected in EXAMPLES:\n",
        "    print(f\"\\n--- Testing: '{cmd}' ‚Üí expected: '{expected}' ---\")\n",
        "    \n",
        "    # Encode source\n",
        "    src = src_vocab.encode(cmd.lower())\n",
        "    src = src[:10] + [0] * (10 - len(src))\n",
        "    src_tensor = torch.tensor([src], device=device)\n",
        "    \n",
        "    print(f\"Source tokens: {src}\")\n",
        "    \n",
        "    # Generate\n",
        "    output = model.generate(src_tensor, max_len=8, verbose=True)\n",
        "    \n",
        "    print(f\"Output tokens: {output[0].tolist()}\")\n",
        "    \n",
        "    # Decode\n",
        "    predicted = tgt_vocab.decode(output[0])\n",
        "    \n",
        "    print(f\"Predicted: '{predicted}'\")\n",
        "    print(f\"Expected:  '{expected}'\")\n",
        "    print(f\"Match: {predicted == expected}\")\n",
        "    \n",
        "    if predicted != expected:\n",
        "        print(f\"  Predicted bytes: {[ord(c) for c in predicted]}\")\n",
        "        print(f\"  Expected bytes:  {[ord(c) for c in expected]}\")"
      ],
      "metadata": {"id": "eval"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CHECK: What does the model predict at each position?\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEACHER FORCING CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test with teacher forcing - should be near perfect\n",
        "model.eval()\n",
        "\n",
        "for cmd, expected in EXAMPLES[:2]:\n",
        "    print(f\"\\n--- '{cmd}' ‚Üí '{expected}' ---\")\n",
        "    \n",
        "    src = src_vocab.encode(cmd.lower())\n",
        "    src = src[:10] + [0] * (10 - len(src))\n",
        "    \n",
        "    tgt = tgt_vocab.encode(expected, add_sos=True, add_eos=True)\n",
        "    tgt = tgt[:10] + [0] * (10 - len(tgt))\n",
        "    \n",
        "    src_tensor = torch.tensor([src], device=device)\n",
        "    tgt_tensor = torch.tensor([tgt], device=device)\n",
        "    \n",
        "    print(f\"Target tokens: {tgt}\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model(src_tensor, tgt_tensor[:, :-1])\n",
        "        predictions = output.argmax(dim=-1)\n",
        "    \n",
        "    print(f\"Predictions:   {predictions[0].tolist()}\")\n",
        "    print(f\"Expected:      {tgt[1:]}\")\n",
        "    \n",
        "    # Check accuracy\n",
        "    tgt_output = tgt_tensor[:, 1:]\n",
        "    mask = tgt_output != 0\n",
        "    correct = (predictions == tgt_output) & mask\n",
        "    acc = correct.sum().item() / mask.sum().item()\n",
        "    print(f\"Token accuracy: {acc:.1%}\")"
      ],
      "metadata": {"id": "teacher_forcing"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL ACCURACY CALCULATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL ACCURACY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "correct = 0\n",
        "total = len(EXAMPLES)\n",
        "\n",
        "for cmd, expected in EXAMPLES:\n",
        "    src = src_vocab.encode(cmd.lower())\n",
        "    src = src[:10] + [0] * (10 - len(src))\n",
        "    src_tensor = torch.tensor([src], device=device)\n",
        "    \n",
        "    output = model.generate(src_tensor, max_len=8, verbose=False)\n",
        "    predicted = tgt_vocab.decode(output[0])\n",
        "    \n",
        "    is_correct = predicted == expected\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    \n",
        "    status = \"‚úì\" if is_correct else \"‚úó\"\n",
        "    print(f\"{status} '{cmd}' ‚Üí '{predicted}' (expected: '{expected}')\")\n",
        "\n",
        "print(f\"\\nAccuracy: {correct}/{total} = {correct/total:.1%}\")"
      ],
      "metadata": {"id": "final"},
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Resonance Protocol ‚Äî M2.5d: Cluster Sampling Strategies\n",
        "\n",
        "**Hypothesis 3:** Different sampling strategies from HDC clusters may improve fine-tuning results.\n",
        "\n",
        "**Strategies to test:**\n",
        "1. **Centroid** (baseline) ‚Äî closest to cluster center (most typical)\n",
        "2. **Boundary** ‚Äî farthest from cluster center (most diverse/difficult)\n",
        "3. **Mixed** ‚Äî 70% centroids + 30% boundary\n",
        "4. **Curriculum** ‚Äî train on centroids first, then boundary\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "!pip install -q sentence-transformers scikit-learn\n",
        "!pip install -q tqdm numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load Data and Create HDC Embeddings"
      ],
      "metadata": {
        "id": "data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load Alpaca\n",
        "print(\"Loading Alpaca dataset...\")\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "\n",
        "POOL_SIZE = 2000\n",
        "SUBSET_SIZE = 500\n",
        "N_CLUSTERS = 500\n",
        "\n",
        "random.seed(42)\n",
        "pool_indices = random.sample(range(len(dataset)), POOL_SIZE)\n",
        "pool = dataset.select(pool_indices)\n",
        "print(f\"Pool size: {len(pool)}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format texts\n",
        "def format_example(example):\n",
        "    text = f\"Instruction: {example['instruction']}\"\n",
        "    if example.get('input'):\n",
        "        text += f\"\\nInput: {example['input']}\"\n",
        "    text += f\"\\nOutput: {example['output']}\"\n",
        "    return text\n",
        "\n",
        "pool_texts = [format_example(ex) for ex in pool]\n",
        "\n",
        "# Encode with SentenceTransformer\n",
        "print(\"\\nEncoding with SentenceTransformer...\")\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "st_embeddings = st_model.encode(pool_texts, show_progress_bar=True)\n",
        "print(f\"ST embeddings shape: {st_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "encode_st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HDC Encoder\n",
        "class TernaryHDCEncoder:\n",
        "    def __init__(self, input_dim=384, hd_dim=10000, sparsity=0.7, seed=42):\n",
        "        self.hd_dim = hd_dim\n",
        "        self.sparsity = sparsity\n",
        "        np.random.seed(seed)\n",
        "        self.projection = np.random.randn(input_dim, hd_dim).astype(np.float32)\n",
        "        self.projection /= np.sqrt(input_dim)\n",
        "    \n",
        "    def encode(self, embeddings):\n",
        "        projected = embeddings @ self.projection\n",
        "        ternary = np.zeros_like(projected)\n",
        "        for i in range(len(projected)):\n",
        "            vec = projected[i]\n",
        "            threshold = np.percentile(np.abs(vec), self.sparsity * 100)\n",
        "            ternary[i] = np.where(vec > threshold, 1,\n",
        "                                   np.where(vec < -threshold, -1, 0))\n",
        "        return ternary\n",
        "\n",
        "# Encode with HDC\n",
        "print(\"\\nEncoding with HDC...\")\n",
        "hdc_encoder = TernaryHDCEncoder()\n",
        "hdc_embeddings = hdc_encoder.encode(st_embeddings)\n",
        "print(f\"HDC embeddings shape: {hdc_embeddings.shape}\")\n",
        "print(f\"Sparsity: {(hdc_embeddings == 0).mean():.1%}\")"
      ],
      "metadata": {
        "id": "encode_hdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create Different Sampling Strategies"
      ],
      "metadata": {
        "id": "strategies_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster in HDC space\n",
        "print(\"Clustering in HDC space...\")\n",
        "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(hdc_embeddings)\n",
        "print(f\"Created {N_CLUSTERS} clusters\")\n",
        "\n",
        "# Calculate distances to centroids for each point\n",
        "distances_to_centroids = pairwise_distances(hdc_embeddings, kmeans.cluster_centers_)\n",
        "\n",
        "# For each point, get distance to its own cluster centroid\n",
        "point_distances = np.array([distances_to_centroids[i, cluster_labels[i]] \n",
        "                            for i in range(len(hdc_embeddings))])\n",
        "\n",
        "print(f\"Distance range: {point_distances.min():.2f} - {point_distances.max():.2f}\")"
      ],
      "metadata": {
        "id": "clustering"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_from_clusters(strategy='centroid', n_samples=500):\n",
        "    \"\"\"\n",
        "    Select samples from clusters using different strategies.\n",
        "    \n",
        "    Strategies:\n",
        "    - 'centroid': closest to cluster center (most typical)\n",
        "    - 'boundary': farthest from cluster center (most diverse)\n",
        "    - 'mixed': 70% centroid + 30% boundary\n",
        "    \"\"\"\n",
        "    selected = []\n",
        "    \n",
        "    for cluster_id in range(N_CLUSTERS):\n",
        "        # Get indices of points in this cluster\n",
        "        cluster_mask = cluster_labels == cluster_id\n",
        "        cluster_indices = np.where(cluster_mask)[0]\n",
        "        \n",
        "        if len(cluster_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Get distances for points in this cluster\n",
        "        cluster_distances = point_distances[cluster_indices]\n",
        "        \n",
        "        if strategy == 'centroid':\n",
        "            # Select closest to centroid\n",
        "            best_idx = cluster_indices[np.argmin(cluster_distances)]\n",
        "            selected.append(best_idx)\n",
        "            \n",
        "        elif strategy == 'boundary':\n",
        "            # Select farthest from centroid\n",
        "            best_idx = cluster_indices[np.argmax(cluster_distances)]\n",
        "            selected.append(best_idx)\n",
        "            \n",
        "        elif strategy == 'mixed':\n",
        "            # Will handle after loop\n",
        "            selected.append({\n",
        "                'centroid': cluster_indices[np.argmin(cluster_distances)],\n",
        "                'boundary': cluster_indices[np.argmax(cluster_distances)]\n",
        "            })\n",
        "    \n",
        "    if strategy == 'mixed':\n",
        "        # 70% centroid, 30% boundary\n",
        "        n_centroid = int(n_samples * 0.7)\n",
        "        n_boundary = n_samples - n_centroid\n",
        "        \n",
        "        random.seed(42)\n",
        "        random.shuffle(selected)\n",
        "        \n",
        "        final_selected = []\n",
        "        for i, item in enumerate(selected):\n",
        "            if i < n_centroid:\n",
        "                final_selected.append(item['centroid'])\n",
        "            else:\n",
        "                final_selected.append(item['boundary'])\n",
        "        \n",
        "        return list(set(final_selected))[:n_samples]\n",
        "    \n",
        "    return list(set(selected))[:n_samples]\n",
        "\n",
        "# Create all strategy subsets\n",
        "centroid_indices = select_from_clusters('centroid', SUBSET_SIZE)\n",
        "boundary_indices = select_from_clusters('boundary', SUBSET_SIZE)\n",
        "mixed_indices = select_from_clusters('mixed', SUBSET_SIZE)\n",
        "\n",
        "print(f\"Centroid subset: {len(centroid_indices)} samples\")\n",
        "print(f\"Boundary subset: {len(boundary_indices)} samples\")\n",
        "print(f\"Mixed subset: {len(mixed_indices)} samples\")"
      ],
      "metadata": {
        "id": "strategies"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create curriculum dataset (centroid first half, boundary second half)\n",
        "# This will be handled during training by concatenating datasets\n",
        "\n",
        "n_half = SUBSET_SIZE // 2\n",
        "curriculum_centroid_indices = centroid_indices[:n_half]\n",
        "curriculum_boundary_indices = boundary_indices[:n_half]\n",
        "\n",
        "print(f\"\\nCurriculum strategy:\")\n",
        "print(f\"  Phase 1 (easy/centroid): {len(curriculum_centroid_indices)} samples\")\n",
        "print(f\"  Phase 2 (hard/boundary): {len(curriculum_boundary_indices)} samples\")"
      ],
      "metadata": {
        "id": "curriculum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the strategies\n",
        "print(\"\\nüìä Strategy Analysis:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for name, indices in [('Centroid', centroid_indices), \n",
        "                       ('Boundary', boundary_indices),\n",
        "                       ('Mixed', mixed_indices)]:\n",
        "    subset_distances = point_distances[indices]\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Mean distance to centroid: {subset_distances.mean():.3f}\")\n",
        "    print(f\"  Std distance: {subset_distances.std():.3f}\")\n",
        "    print(f\"  Min/Max: {subset_distances.min():.3f} / {subset_distances.max():.3f}\")"
      ],
      "metadata": {
        "id": "analyze_strategies"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Prepare Datasets"
      ],
      "metadata": {
        "id": "prepare_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset subsets\n",
        "centroid_subset = pool.select(centroid_indices)\n",
        "boundary_subset = pool.select(boundary_indices)\n",
        "mixed_subset = pool.select(mixed_indices)\n",
        "\n",
        "# For curriculum: concatenate centroid first, then boundary\n",
        "from datasets import concatenate_datasets\n",
        "curriculum_phase1 = pool.select(curriculum_centroid_indices)\n",
        "curriculum_phase2 = pool.select(curriculum_boundary_indices)\n",
        "curriculum_subset = concatenate_datasets([curriculum_phase1, curriculum_phase2])\n",
        "\n",
        "print(f\"Centroid subset: {len(centroid_subset)}\")\n",
        "print(f\"Boundary subset: {len(boundary_subset)}\")\n",
        "print(f\"Mixed subset: {len(mixed_subset)}\")\n",
        "print(f\"Curriculum subset: {len(curriculum_subset)}\")"
      ],
      "metadata": {
        "id": "create_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Load Model"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_function(examples):\n",
        "    texts = []\n",
        "    for i in range(len(examples['instruction'])):\n",
        "        text = f\"### Instruction:\\n{examples['instruction'][i]}\\n\\n\"\n",
        "        if examples['input'][i]:\n",
        "            text += f\"### Input:\\n{examples['input'][i]}\\n\\n\"\n",
        "        text += f\"### Response:\\n{examples['output'][i]}\"\n",
        "        texts.append(text)\n",
        "    \n",
        "    tokenized = tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "centroid_tokenized = centroid_subset.map(tokenize_function, batched=True, remove_columns=centroid_subset.column_names)\n",
        "boundary_tokenized = boundary_subset.map(tokenize_function, batched=True, remove_columns=boundary_subset.column_names)\n",
        "mixed_tokenized = mixed_subset.map(tokenize_function, batched=True, remove_columns=mixed_subset.column_names)\n",
        "curriculum_tokenized = curriculum_subset.map(tokenize_function, batched=True, remove_columns=curriculum_subset.column_names)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "tokenize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Training"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(train_dataset, run_name):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {run_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    model = get_peft_model(base_model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{run_name}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        warmup_steps=50,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"no\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "    \n",
        "    trainer.train()\n",
        "    \n",
        "    loss_history = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
        "    print(f\"\\nFinal loss: {loss_history[-1]:.4f}\")\n",
        "    \n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return {\n",
        "        'name': run_name,\n",
        "        'final_loss': loss_history[-1],\n",
        "        'loss_history': loss_history,\n",
        "        'train_samples': len(train_dataset)\n",
        "    }"
      ],
      "metadata": {
        "id": "train_function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all experiments\n",
        "results = {}\n",
        "\n",
        "results['centroid'] = train_and_evaluate(centroid_tokenized, 'centroid')\n",
        "results['boundary'] = train_and_evaluate(boundary_tokenized, 'boundary')\n",
        "results['mixed'] = train_and_evaluate(mixed_tokenized, 'mixed')\n",
        "results['curriculum'] = train_and_evaluate(curriculum_tokenized, 'curriculum')"
      ],
      "metadata": {
        "id": "run_experiments"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Results"
      ],
      "metadata": {
        "id": "results_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "colors = {'centroid': 'green', 'boundary': 'red', 'mixed': 'purple', 'curriculum': 'orange'}\n",
        "\n",
        "for name, data in results.items():\n",
        "    plt.plot(data['loss_history'], label=f\"{name} (final: {data['final_loss']:.4f})\", \n",
        "             color=colors[name], linewidth=2)\n",
        "\n",
        "plt.xlabel('Training Steps (√ó10)')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('M2.5d: Cluster Sampling Strategies\\nCentroid vs Boundary vs Mixed vs Curriculum')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('m2.5d_strategies.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä M2.5d RESULTS: CLUSTER SAMPLING STRATEGIES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Previous best (HDC-Curated from M2.5c)\n",
        "PREV_BEST = 1.2194\n",
        "\n",
        "print(f\"\\n{'Strategy':<15} {'Final Loss':>12} {'vs M2.5c Best':>15} {'Status':>10}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['final_loss'])\n",
        "\n",
        "for name, data in sorted_results:\n",
        "    loss = data['final_loss']\n",
        "    vs_prev = ((PREV_BEST - loss) / PREV_BEST) * 100\n",
        "    status = \"üëë NEW BEST\" if loss < PREV_BEST else \"\" \n",
        "    print(f\"{name:<15} {loss:>12.4f} {vs_prev:>+14.2f}% {status:>10}\")\n",
        "\n",
        "print(f\"\\nPrevious best (M2.5c HDC-Curated): {PREV_BEST}\")\n",
        "\n",
        "# Winner\n",
        "winner_name, winner_data = sorted_results[0]\n",
        "print(f\"\\nüèÜ Winner: {winner_name.upper()} with loss {winner_data['final_loss']:.4f}\")\n",
        "\n",
        "# Analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üî¨ ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "centroid_loss = results['centroid']['final_loss']\n",
        "boundary_loss = results['boundary']['final_loss']\n",
        "mixed_loss = results['mixed']['final_loss']\n",
        "curriculum_loss = results['curriculum']['final_loss']\n",
        "\n",
        "print(f\"\\nCentroid vs Boundary: {((boundary_loss - centroid_loss) / boundary_loss) * 100:+.2f}%\")\n",
        "print(f\"Mixed vs Centroid: {((centroid_loss - mixed_loss) / centroid_loss) * 100:+.2f}%\")\n",
        "print(f\"Curriculum vs Centroid: {((centroid_loss - curriculum_loss) / centroid_loss) * 100:+.2f}%\")"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "import json\n",
        "\n",
        "output = {\n",
        "    \"phase\": \"M2.5d\",\n",
        "    \"experiment\": \"Cluster Sampling Strategies\",\n",
        "    \"hypothesis\": \"Different sampling strategies from HDC clusters may improve fine-tuning\",\n",
        "    \"previous_best\": PREV_BEST,\n",
        "    \"results\": {\n",
        "        name: {\n",
        "            \"final_loss\": float(data['final_loss']),\n",
        "            \"samples\": data['train_samples'],\n",
        "            \"vs_prev_best_pct\": float(((PREV_BEST - data['final_loss']) / PREV_BEST) * 100)\n",
        "        }\n",
        "        for name, data in results.items()\n",
        "    },\n",
        "    \"winner\": winner_name,\n",
        "    \"winner_loss\": float(winner_data['final_loss']),\n",
        "    \"improved_over_m2.5c\": winner_data['final_loss'] < PREV_BEST\n",
        "}\n",
        "\n",
        "with open('phase_m2.5d_results.json', 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"\\nüìÅ Results saved to phase_m2.5d_results.json\")\n",
        "print(\"\\n\" + json.dumps(output, indent=2))"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "from google.colab import files\n",
        "files.download('phase_m2.5d_results.json')\n",
        "files.download('m2.5d_strategies.png')"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

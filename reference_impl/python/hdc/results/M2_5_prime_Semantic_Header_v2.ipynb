{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ M2.5‚Ä≤: HDC Semantic Header\n",
        "\n",
        "**Hypothesis:** HDC vector can improve model inference by providing semantic context as pseudo-tokens.\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "[Text] ‚Üí HDC Encoder ‚Üí [10,000 ternary]\n",
        "                            ‚Üì\n",
        "                     MLP (10,000 ‚Üí k √ó hidden_dim)\n",
        "                            ‚Üì\n",
        "                    [k pseudo-tokens]\n",
        "                            ‚Üì\n",
        "         [pseudo-tokens] + [text embeddings] ‚Üí LLM ‚Üí output\n",
        "```\n",
        "\n",
        "**Task:** Sentiment classification (SST-2)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q tqdm numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load Model First (to get HIDDEN_SIZE)"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use smaller model for faster loading\n",
        "MODEL_NAME = \"facebook/opt-350m\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "# Load without quantization - model is small enough\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Get hidden size\n",
        "HIDDEN_SIZE = model.config.hidden_size\n",
        "print(f\"Model loaded! Hidden size: {HIDDEN_SIZE}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: HDC Encoder"
      ],
      "metadata": {
        "id": "hdc_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TernaryHDCEncoder:\n",
        "    \"\"\"Encode text to ternary HDC vectors via SentenceTransformer + projection\"\"\"\n",
        "    \n",
        "    def __init__(self, hd_dim=10000, sparsity=0.7, seed=42):\n",
        "        self.hd_dim = hd_dim\n",
        "        self.sparsity = sparsity\n",
        "        self.st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.st_dim = 384\n",
        "        \n",
        "        # Fixed random projection (float32)\n",
        "        np.random.seed(seed)\n",
        "        self.projection = torch.tensor(\n",
        "            np.random.randn(self.st_dim, hd_dim).astype(np.float32) / np.sqrt(self.st_dim),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "    \n",
        "    def encode(self, texts):\n",
        "        \"\"\"Encode list of texts to ternary HDC vectors\"\"\"\n",
        "        # Get ST embeddings\n",
        "        st_emb = self.st_model.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n",
        "        \n",
        "        # Ensure float32\n",
        "        st_emb = st_emb.float()\n",
        "        projection = self.projection.to(st_emb.device).float()\n",
        "        \n",
        "        # Project to HD space\n",
        "        projected = st_emb @ projection\n",
        "        \n",
        "        # Ternarize\n",
        "        ternary = torch.zeros_like(projected)\n",
        "        for i in range(len(projected)):\n",
        "            vec = projected[i]\n",
        "            threshold = torch.quantile(torch.abs(vec), self.sparsity)\n",
        "            ternary[i] = torch.where(vec > threshold, torch.ones_like(vec),\n",
        "                                      torch.where(vec < -threshold, -torch.ones_like(vec),\n",
        "                                                  torch.zeros_like(vec)))\n",
        "        \n",
        "        return ternary\n",
        "\n",
        "# Test\n",
        "hdc_encoder = TernaryHDCEncoder()\n",
        "test_hdc = hdc_encoder.encode([\"This is a test sentence.\"])\n",
        "print(f\"HDC shape: {test_hdc.shape}\")\n",
        "print(f\"Sparsity: {(test_hdc == 0).float().mean():.1%}\")"
      ],
      "metadata": {
        "id": "hdc_encoder"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Semantic Header Module"
      ],
      "metadata": {
        "id": "header_module"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticHeader(nn.Module):\n",
        "    \"\"\"\n",
        "    Converts HDC vector to k pseudo-tokens that can be prepended to input embeddings.\n",
        "    \n",
        "    HDC (10,000) ‚Üí MLP ‚Üí k √ó hidden_dim\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, hdc_dim=10000, hidden_dim=512, n_tokens=4):\n",
        "        super().__init__()\n",
        "        self.n_tokens = n_tokens\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # MLP: HDC ‚Üí intermediate ‚Üí n_tokens √ó hidden_dim\n",
        "        intermediate_dim = min(2048, hidden_dim * 4)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hdc_dim, intermediate_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(intermediate_dim, n_tokens * hidden_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, hdc_vectors):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hdc_vectors: (batch_size, hdc_dim)\n",
        "        \n",
        "        Returns:\n",
        "            pseudo_tokens: (batch_size, n_tokens, hidden_dim)\n",
        "        \"\"\"\n",
        "        batch_size = hdc_vectors.shape[0]\n",
        "        output = self.mlp(hdc_vectors)\n",
        "        pseudo_tokens = output.view(batch_size, self.n_tokens, self.hidden_dim)\n",
        "        return pseudo_tokens\n",
        "\n",
        "# Test with actual HIDDEN_SIZE\n",
        "print(f\"Model hidden size: {HIDDEN_SIZE}\")\n",
        "header = SemanticHeader(hdc_dim=10000, hidden_dim=HIDDEN_SIZE, n_tokens=4).to(device)\n",
        "test_tokens = header(test_hdc.float().to(device))\n",
        "print(f\"Pseudo-tokens shape: {test_tokens.shape}\")"
      ],
      "metadata": {
        "id": "semantic_header"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Load Dataset"
      ],
      "metadata": {
        "id": "data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SST-2\n",
        "print(\"Loading SST-2 dataset...\")\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "# Limit for speed\n",
        "MAX_TRAIN = 2000\n",
        "MAX_VAL = 500\n",
        "\n",
        "train_texts = dataset['train']['sentence'][:MAX_TRAIN]\n",
        "train_labels = dataset['train']['label'][:MAX_TRAIN]\n",
        "val_texts = dataset['validation']['sentence'][:MAX_VAL]\n",
        "val_labels = dataset['validation']['label'][:MAX_VAL]\n",
        "\n",
        "print(f\"Train: {len(train_texts)} samples\")\n",
        "print(f\"Val: {len(val_texts)} samples\")\n",
        "print(f\"Labels: 0=negative, 1=positive\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-compute HDC vectors for all texts\n",
        "print(\"\\nEncoding texts to HDC...\")\n",
        "\n",
        "def batch_encode_hdc(texts, encoder, batch_size=64):\n",
        "    all_vectors = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        vectors = encoder.encode(batch)\n",
        "        all_vectors.append(vectors.cpu())\n",
        "    return torch.cat(all_vectors, dim=0)\n",
        "\n",
        "train_hdc = batch_encode_hdc(train_texts, hdc_encoder)\n",
        "val_hdc = batch_encode_hdc(val_texts, hdc_encoder)\n",
        "\n",
        "print(f\"Train HDC shape: {train_hdc.shape}\")\n",
        "print(f\"Val HDC shape: {val_hdc.shape}\")"
      ],
      "metadata": {
        "id": "encode_hdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Custom Dataset & Model"
      ],
      "metadata": {
        "id": "custom_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, hdc_vectors, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.hdc_vectors = hdc_vectors\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = f\"Classify the sentiment of this text as positive or negative.\\nText: {self.texts[idx]}\\nSentiment:\"\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'hdc_vector': self.hdc_vectors[idx],\n",
        "            'label': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, train_hdc, tokenizer)\n",
        "val_dataset = SentimentDataset(val_texts, val_labels, val_hdc, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "id": "dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifierWithHeader(nn.Module):\n",
        "    \"\"\"\n",
        "    Sentiment classifier with optional HDC semantic header.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, base_model, hidden_size, use_header=True, n_tokens=4):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.use_header = use_header\n",
        "        self.n_tokens = n_tokens\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        if use_header:\n",
        "            self.semantic_header = SemanticHeader(\n",
        "                hdc_dim=10000,\n",
        "                hidden_dim=hidden_size,\n",
        "                n_tokens=n_tokens\n",
        "            )\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, hdc_vector=None):\n",
        "        # Get input embeddings\n",
        "        inputs_embeds = self.base_model.get_input_embeddings()(input_ids)\n",
        "        \n",
        "        if self.use_header and hdc_vector is not None:\n",
        "            # Get pseudo-tokens from HDC\n",
        "            pseudo_tokens = self.semantic_header(hdc_vector.float())\n",
        "            pseudo_tokens = pseudo_tokens.to(inputs_embeds.dtype)\n",
        "            \n",
        "            # Prepend pseudo-tokens\n",
        "            inputs_embeds = torch.cat([pseudo_tokens, inputs_embeds], dim=1)\n",
        "            \n",
        "            # Extend attention mask\n",
        "            batch_size = attention_mask.shape[0]\n",
        "            header_mask = torch.ones(batch_size, self.n_tokens, device=attention_mask.device)\n",
        "            attention_mask = torch.cat([header_mask, attention_mask], dim=1)\n",
        "        \n",
        "        # Forward through LLM\n",
        "        outputs = self.base_model(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        \n",
        "        # Get last hidden state\n",
        "        last_hidden = outputs.hidden_states[-1][:, -1, :]\n",
        "        last_hidden = last_hidden.float()\n",
        "        \n",
        "        # Classify\n",
        "        logits = self.classifier(last_hidden)\n",
        "        \n",
        "        return logits"
      ],
      "metadata": {
        "id": "model_with_header"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Training Functions"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        hdc_vector = batch['hdc_vector'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logits = model(input_ids, attention_mask, hdc_vector)\n",
        "        loss = criterion(logits, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    \n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            hdc_vector = batch['hdc_vector'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            logits = model(input_ids, attention_mask, hdc_vector)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    \n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "training_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(use_header, n_tokens=4, epochs=3, lr=1e-4):\n",
        "    \"\"\"\n",
        "    Run full training experiment.\n",
        "    \"\"\"\n",
        "    name = f\"HDC Header (k={n_tokens})\" if use_header else \"Baseline (no header)\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Experiment: {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Create model\n",
        "    classifier = SentimentClassifierWithHeader(\n",
        "        base_model=model,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        use_header=use_header,\n",
        "        n_tokens=n_tokens\n",
        "    ).to(device)\n",
        "    \n",
        "    # Freeze base model\n",
        "    for param in classifier.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Count trainable params\n",
        "    trainable = sum(p.numel() for p in classifier.parameters() if p.requires_grad)\n",
        "    print(f\"Trainable parameters: {trainable:,}\")\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, classifier.parameters()),\n",
        "        lr=lr\n",
        "    )\n",
        "    \n",
        "    # Training loop\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "    best_val_acc = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        \n",
        "        train_loss, train_acc = train_epoch(classifier, train_loader, optimizer, device)\n",
        "        val_acc = evaluate(classifier, val_loader, device)\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "        \n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Best Val Accuracy: {best_val_acc:.4f}\")\n",
        "    \n",
        "    # Cleanup\n",
        "    del classifier\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return {\n",
        "        'name': name,\n",
        "        'use_header': use_header,\n",
        "        'n_tokens': n_tokens if use_header else 0,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'final_val_acc': history['val_acc'][-1],\n",
        "        'history': history,\n",
        "        'trainable_params': trainable\n",
        "    }"
      ],
      "metadata": {
        "id": "run_experiment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Run Experiments"
      ],
      "metadata": {
        "id": "experiments_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "# 1. Baseline (no header)\n",
        "results['baseline'] = run_experiment(use_header=False, epochs=3)\n",
        "\n",
        "# 2. HDC Header with k=2\n",
        "results['header_k2'] = run_experiment(use_header=True, n_tokens=2, epochs=3)\n",
        "\n",
        "# 3. HDC Header with k=4\n",
        "results['header_k4'] = run_experiment(use_header=True, n_tokens=4, epochs=3)\n",
        "\n",
        "# 4. HDC Header with k=8\n",
        "results['header_k8'] = run_experiment(use_header=True, n_tokens=8, epochs=3)"
      ],
      "metadata": {
        "id": "run_experiments"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Results"
      ],
      "metadata": {
        "id": "results_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "colors = {'baseline': 'gray', 'header_k2': 'green', 'header_k4': 'blue', 'header_k8': 'red'}\n",
        "\n",
        "# Validation accuracy\n",
        "ax = axes[0]\n",
        "for name, data in results.items():\n",
        "    ax.plot(data['history']['val_acc'], label=f\"{data['name']} (best: {data['best_val_acc']:.4f})\",\n",
        "            color=colors[name], linewidth=2, marker='o')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Validation Accuracy')\n",
        "ax.set_title('Validation Accuracy by Epoch')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Training loss\n",
        "ax = axes[1]\n",
        "for name, data in results.items():\n",
        "    ax.plot(data['history']['train_loss'], label=data['name'],\n",
        "            color=colors[name], linewidth=2, marker='o')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Training Loss')\n",
        "ax.set_title('Training Loss by Epoch')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('m2.5_prime_results.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plot_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary table\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä M2.5‚Ä≤ RESULTS: HDC SEMANTIC HEADER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "baseline_acc = results['baseline']['best_val_acc']\n",
        "\n",
        "print(f\"\\n{'Config':<30} {'Val Accuracy':>15} {'vs Baseline':>15} {'Params':>15}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['best_val_acc'], reverse=True)\n",
        "\n",
        "for name, data in sorted_results:\n",
        "    acc = data['best_val_acc']\n",
        "    vs_baseline = ((acc - baseline_acc) / baseline_acc) * 100\n",
        "    status = \"üèÜ\" if acc > baseline_acc else \"\"\n",
        "    print(f\"{data['name']:<30} {acc:>15.4f} {vs_baseline:>+14.2f}% {data['trainable_params']:>15,} {status}\")\n",
        "\n",
        "# Find best\n",
        "best_name, best_data = sorted_results[0]\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"üî¨ ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if best_data['use_header']:\n",
        "    improvement = ((best_data['best_val_acc'] - baseline_acc) / baseline_acc) * 100\n",
        "    print(f\"\\n‚úÖ HDC Semantic Header IMPROVES accuracy by {improvement:.2f}%\")\n",
        "    print(f\"   Best config: {best_data['name']}\")\n",
        "    print(f\"   Accuracy: {best_data['best_val_acc']:.4f} vs baseline {baseline_acc:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è HDC Semantic Header did NOT improve over baseline\")\n",
        "    print(f\"   Baseline accuracy: {baseline_acc:.4f}\")\n",
        "\n",
        "# Verdict\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"üìã VERDICT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "header_results = [r for r in results.values() if r['use_header']]\n",
        "best_header = max(header_results, key=lambda x: x['best_val_acc'])\n",
        "\n",
        "if best_header['best_val_acc'] > baseline_acc * 1.01:  # >1% improvement\n",
        "    print(\"\\n‚úÖ SUCCESS: HDC Semantic Header provides meaningful improvement\")\n",
        "    verdict = \"SUCCESS\"\n",
        "elif best_header['best_val_acc'] >= baseline_acc * 0.995:  # Within 0.5%\n",
        "    print(\"\\n‚ö†Ô∏è PARTIAL: HDC Semantic Header matches baseline (no degradation)\")\n",
        "    verdict = \"PARTIAL\"\n",
        "else:\n",
        "    print(\"\\n‚ùå FAILURE: HDC Semantic Header hurts performance\")\n",
        "    verdict = \"FAILURE\""
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "import json\n",
        "\n",
        "output = {\n",
        "    \"phase\": \"M2.5_prime\",\n",
        "    \"experiment\": \"HDC Semantic Header\",\n",
        "    \"hypothesis\": \"HDC vector as pseudo-tokens improves classification\",\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"dataset\": \"SST-2\",\n",
        "    \"train_samples\": MAX_TRAIN,\n",
        "    \"val_samples\": MAX_VAL,\n",
        "    \"results\": {\n",
        "        name: {\n",
        "            \"name\": data['name'],\n",
        "            \"use_header\": data['use_header'],\n",
        "            \"n_tokens\": data['n_tokens'],\n",
        "            \"best_val_acc\": float(data['best_val_acc']),\n",
        "            \"final_val_acc\": float(data['final_val_acc']),\n",
        "            \"trainable_params\": data['trainable_params'],\n",
        "            \"vs_baseline_pct\": float(((data['best_val_acc'] - baseline_acc) / baseline_acc) * 100)\n",
        "        }\n",
        "        for name, data in results.items()\n",
        "    },\n",
        "    \"baseline_accuracy\": float(baseline_acc),\n",
        "    \"best_header_accuracy\": float(best_header['best_val_acc']),\n",
        "    \"best_header_config\": best_header['name'],\n",
        "    \"improvement_pct\": float(((best_header['best_val_acc'] - baseline_acc) / baseline_acc) * 100),\n",
        "    \"verdict\": verdict\n",
        "}\n",
        "\n",
        "with open('phase_m2.5_prime_results.json', 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"\\nüìÅ Results saved to phase_m2.5_prime_results.json\")\n",
        "print(\"\\n\" + json.dumps(output, indent=2))"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "from google.colab import files\n",
        "files.download('phase_m2.5_prime_results.json')\n",
        "files.download('m2.5_prime_results.png')"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
